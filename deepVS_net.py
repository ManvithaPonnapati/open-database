import tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#---------------------------------HYPERPARAMETERS---------------------------------#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#atom type embedding size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFd_atm = 200123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#amino acid embedding size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFd_amino = 200123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#charge embedding size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFd_chrg = 200123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#distance embedding size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFd_dist = 200123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#number convolutional filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFcf = 400123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#number hidden units123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFh = 50123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#learning rate123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFl = 0.075123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#number of neighbor atoms from ligand123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFk_c = 6123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#number of neighbor atoms from protein123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFk_p = 0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#number of atom types123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFATOM_TYPES = 7123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#number of distance bins123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFDIST_BINS = 18123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFDIST_INTERVAL = 0.3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#-------------------------------LAYER CONSTRUCTION--------------------------------#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# telling tensorflow how we want to randomly initialize weights123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef weight_variable(shape):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = tf.truncated_normal(shape, stddev=0.005)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return tf.Variable(initial)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef bias_variable(shape):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = tf.constant(0.01, shape=shape)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return tf.Variable(initial)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef variable_summaries(var, name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """attaches a lot of summaries to a tensor."""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('summaries'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        mean = tf.reduce_mean(var)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('mean/' + name, mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('stddev'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('stddev/' + name, stddev)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('max/' + name, tf.reduce_max(var))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('min/' + name, tf.reduce_min(var))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		tf.summary.histogram(name, var)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef preprocess_layer(layer_name, atoms, coords):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#get a matrix of distances between atoms (tensor with shape [m, m])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	coords_copy = tf.transpose(tf.expand_dims(coords, 0), perm=[1,0,2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	distances = tf.reduce_sum(tf.square(coords - coords_copy), reduction_indices=[2]) ** 0.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#get the closest neighbors using tf.nn.top_k, and the corresponding atoms123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	neg_distances = tf.multiply(distances, tf.constant(-1.0))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	neighbor_neg_distances, neighbor_atom_indices = tf.nn.top_k(neg_distances, k_c)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#binning the distances and associating atom indices with actual atom types123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	neighbor_distances = tf.multiply(neighbor_neg_distances, tf.constant(-1.0/DIST_INTERVAL))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	neighbor_distances = tf.add(neighbor_distances, tf.constant(1.0))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	neighbor_atoms = tf.gather(atoms, neighbor_atom_indices)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#convert to int, since we'll need that for tf.nn.embedding_lookup123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	neighbor_distances = tf.to_int32(neighbor_distances)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#combine the neighbor atoms and neighbor distances123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z = tf.stack([neighbor_atoms, neighbor_distances], axis=1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#reshape our tensor to get shape [k_c, 2, m]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z_transposed = tf.transpose(z, perm=[2, 1, 0])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	print layer_name, "output dimensions:", z_transposed.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	return z_transposed123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef embed_layer(layer_name, input_tensor):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	"""performs feature embedding on the input tensor, 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	transforming numbers to 200D vectors.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	Input: tensor of shape [k_c, 2, m]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	Output: tensor of shape [k_c, d_dist + d_atm, m, 1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	with tf.name_scope(layer_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		with tf.name_scope('atom_weights'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF			W_atom = weight_variable([ATOM_TYPES, d_atm])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF			variable_summaries(W_atom, layer_name + '/atom_weights')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		with tf.name_scope('dist_weights'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF			W_dist = weight_variable([DIST_BINS, d_dist])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF			variable_summaries(W_dist, layer_name + '/dist_weights')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		h_embed = tf.nn.embedding_lookup([W_atom, W_dist], input_tensor, name='embed_layer')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		h_embed = tf.transpose(h_embed, perm=[0, 1, 3, 2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		h_embed = tf.reshape(h_embed, [1, k_c, d_atm+d_dist, -1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		tf.summary.histogram(layer_name + '/embed_output', h_embed)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	print layer_name, "output dimensions:", h_embed.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	return h_embed123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef conv_layer(layer_name, input_tensor, filter_size, strides=[1,1,1,1,1], padding='SAME'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	"""makes a simple face convolutional layer"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	with tf.name_scope(layer_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		with tf.name_scope('weights'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF			W_conv = weight_variable(filter_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF			variable_summaries(W_conv, layer_name + '/weights')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		with tf.name_scope('biases'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF			b_conv = bias_variable([filter_size[3]])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF			variable_summaries(b_conv, layer_name + '/biases')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		h_conv = tf.nn.conv3d(input_tensor, W_conv, strides=strides, padding=padding) + b_conv123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		tf.summary.histogram(layer_name + 'conv_output', h_conv)		123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	print layer_name,"output dimensions:", h_conv.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	return h_conv123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef fc_layer(layer_name,input_tensor,output_dim):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	"""makes a simple fully connected layer"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	input_dim = int((input_tensor.get_shape())[1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	with tf.name_scope(layer_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		weights = weight_variable([input_dim, output_dim])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		variable_summaries(weights, layer_name + '/weights')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	with tf.name_scope('biases'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		biases = bias_variable([output_dim])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		variable_summaries(biases, layer_name + '/biases')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	with tf.name_scope('Wx_plus_b'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF		h_fc = tf.matmul(input_tensor, weights) + biases123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	tf.summary.histogram(layer_name + '/fc_output', h_fc)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	print layer_name, "output dimensions:", h_fc.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	return h_fc123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#-----------------------------------NETWORK----------------------------------------#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef deepVS_net(ligand_atoms, ligand_coords, keep_prob):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	"""ligand_atoms has dimensions [m], ligand_coords has dimensions [m, 3]"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#take ligand_atoms, ligand_coords, preprocess it into our z vector123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z_processed = preprocess_layer('preprocess', ligand_atoms, ligand_coords)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#do the feature embedding to get a tensor with shape [k_c, d_dist+d_atm, m, 1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z_embed = embed_layer('embedding', z_processed)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#convolutional layer - padding = 'VALID' prevents 0 padding123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z_conv = conv_layer('face_conv', input_tensor=z_embed, filter_size=[k_c, d_atm+d_dist, 1, 1, cf], padding='VALID')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#max pool along the columns (corresponding to each convolutional filter)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z_pool = tf.reduce_max(z_conv, axis=[3], keep_dims=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#pool gives us batch*1*1*1*cf tensor; flatten it to get a tensor of length cf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#NOTE: THIS IS ACTUALLY 2D in batch!123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z_flattened = tf.reshape(z_pool, [-1, cf])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#fully connected layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z_fc1 = fc_layer(layer_name='fc1', input_tensor=z_flattened, output_dim=h)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#dropout123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#output layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z_output = fc_layer(layer_name='out_neuron', input_tensor=z_fc1, output_dim=2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	#get rid of the batch dimension 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	z_labels = tf.reshape(z_output, [2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF	return z_labels