import time,os123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport pandas as pd123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport re123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4_input import image_and_label_queue,index_the_database_into_queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4_main import FLAGS123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4_networks import intuit_net123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom collections import defaultdict123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.saved_session = './summaries/36_netstate/saved_state-23999'123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.predictions_file_path = re.sub("netstate","logs",FLAGS.saved_session)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.database_path = '../datasets/unlabeled_av4'123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.num_epochs = 25123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.top_k = FLAGS.num_epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass store_predictions:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    store add of the prediction results123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    raw_predictions = defaultdict(list)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    processed_predictions = defaultdict(list)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def add_batch(self, ligand_file_paths, batch_current_epoch, batch_predictions):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        ligand_file_name = map(lambda filename:os.path.basename(filename).split('.')[0],ligand_file_paths)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for ligand,current_epoch,prediction in zip(ligand_file_name, batch_current_epoch, batch_predictions):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #   if in_the_range:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.raw_predictions[ligand].append(prediction)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def reduce(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if a ligand has more than one predictions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        use mean as final predictions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for key, value in self.raw_predictions.items():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if len(value) > 1:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                predictions_size = map(lambda x: len(x), value)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                if len(set(predictions_size)) > 1:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    raise Exception(key, " has different number of predictions ", set(predictions_size))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                self.processed_predictions[key].append(np.mean(value, axis=0))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                self.processed_predictions[key].append(value)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def final_predictions(self, predictions_list):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        length = min(len(predictions_list, 10))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return np.mean(predictions_list[:length])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def fill_na(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for key in self.raw_predictions.keys():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            value_len = len(self.raw_predictions[key])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if value_len>FLAGS.top_k:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                print "{} have more predictions than expected, {} reuqired {} found.".format(key,FLAGS.top_k,value_len)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                for i in range(FLAGS.top_k-value_len):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    self.raw_predictions[key]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def save_multiframe_predictions(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        records = []123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for key, value in self.raw_predictions.items():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            value_len = len(self.raw_predictions[key])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if value_len>FLAGS.top_k:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                print "{} have more predictions than expected, {} reuqired {} found.".format(key,FLAGS.top_k,value_len)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                records.append([key]+value[:FLAGS.top_k])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                records.append([key]+value)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        submission_csv = pd.DataFrame(records, columns=['Id']+[ 'Predicted_%d'%i for i in range(1,len(records[0]))])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        submission_csv.to_csv(FLAGS.predictions_file_path + '_multiframe_submission.csv', index=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def save_average(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        take average of multiple predcition123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :return:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        records = []123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for key,value in self.raw_predictions.items():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            records.append([key,np.mean(np.array(value))])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        submission_csv = pd.DataFrame(records,columns=['ID','Predicted'])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        submission_csv.to_csv(FLAGS.predictions_file_path+'_average_submission.csv',index=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def save_max(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        records = []123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for key,value in self.raw_predictions.items():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            records.append([key, np.max(np.array(value))])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        submission_csv = pd.DataFrame(records, columns=['ID', 'Predicted'])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        submission_csv.to_csv(FLAGS.predictions_file_path + '_max_submission.csv', index=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def save(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.save_average()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.save_max()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.save_multiframe_predictions()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef evaluate_on_train_set():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    "train a network"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create session which all the evaluation happens in123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    sess = tf.Session()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create a filename queue first123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    filename_queue, examples_in_database = index_the_database_into_queue(FLAGS.database_path, shuffle=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create an epoch counter123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # there is an additional step with variable initialization in order to get the name of "count up to" in the graph123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    batch_counter = tf.Variable(0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    sess.run(tf.global_variables_initializer())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    batch_counter_increment = tf.assign(batch_counter,tf.Variable(0).count_up_to(np.round((examples_in_database*FLAGS.num_epochs)/FLAGS.batch_size)))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    batch_counter_var_name = sess.run(tf.report_uninitialized_variables())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    epoch_counter = tf.div(batch_counter*FLAGS.batch_size,examples_in_database)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create a custom shuffle queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ligand_file,current_epoch,label_batch,sparse_image_batch = image_and_label_queue(batch_size=FLAGS.batch_size, pixel_size=FLAGS.pixel_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                          side_pixels=FLAGS.side_pixels, num_threads=FLAGS.num_threads,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                          filename_queue=filename_queue, epoch_counter=epoch_counter)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    image_batch = tf.sparse_tensor_to_dense(sparse_image_batch,validate_indices=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    keep_prob = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    y_conv = intuit_net(image_batch,keep_prob,FLAGS.batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # compute softmax over raw predictions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    predictions = tf.nn.softmax(y_conv)[:,1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # restore variables from sleep123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    saver = tf.train.Saver()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    saver.restore(sess,FLAGS.saved_session)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # use123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    sess.run(tf.contrib.framework.get_variables_by_name(batch_counter_var_name[0])[0].initializer)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    coord = tf.train.Coordinator()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    threads = tf.train.start_queue_runners(sess = sess,coord=coord)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create an instance of a class to store predictions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    all_predictios = store_predictions()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print "starting evalution..."123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    try:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        while True or not coord.should_stop():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            start = time.time()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            batch_num = sess.run([batch_counter_increment])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            test_ligand, test_epoch, test_predictions = sess.run([ligand_file, current_epoch, predictions],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 feed_dict={keep_prob: 1})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "current_epoch:", test_epoch[0], "batch_num:", batch_num,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "\tprediction averages:",np.mean(test_predictions),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "\texamples per second:", "%.2f" % (FLAGS.batch_size / (time.time() - start))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            all_predictios.add_batch(test_ligand, test_epoch, test_predictions)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    except Exception:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "exiting the loop"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    all_predictios.save()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFevaluate_on_train_set()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFprint "All Done"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF