import time,os123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport pandas as pd123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport re123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4_input import image_and_label_queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4 import FLAGS,max_net123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom collections import defaultdict123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.saved_session = './summaries/1_netstate/saved_state-9000'123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.predictions_file_path = re.sub("netstate","logs",FLAGS.saved_session)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass store_predictions:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    store add of the prediction results123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    raw_predictions = defaultdict(list)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    reduce_predictions = defaultdict(list)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def add_batch(self,ligand_file_path,batch_predictions):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        ligand_file_name = os.path.basename(ligand_file_path).split('.')[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.raw_predictions[ligand_file_name].append(batch_predictions)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def reduce(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if a ligand has more than one predictions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        use mean as final predictions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        '''123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for key,value in raw_predictions.items():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if len(value)>1:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                predictions_size = map(lambda x:len(x),value)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                if len(set(predictions_size))>1:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    raise Exception(key," has different number of predictions ",set(predictions_size))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                reduce_predictions[key].append(np.mean(value,axis=0))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                reduce_predictions[key].append(value)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def final_predictions(self,predictions_list):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        length = min(len(predictions_list,10))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return np.mean(predictions_list[:length])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def save_predictions(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        records =[]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for key,value in self.reduce_predictions.items():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            records.append(key,self.final_predictions(value))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        submission_csv = pd.DataFrame(records,columns=['Id','Predicted'])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        submission_csv.to_csv(FLAGS.predictions_file_path+'_submission.csv',index=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def save(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.reduce()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.save_predictions()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef evaluate_on_train_set():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    "train a network"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create session which all the evaluation happens in123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    sess = tf.Session()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ligand_file_name,(_,y_,x_image_batch) = image_and_label_queue(sess = sess,batch_size=FLAGS.batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                               pixel_size=FLAGS.pixel_size,side_pixels=FLAGS.side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                               num_threads=FLAGS.num_threads,database_path=FLAGS.test_set_path)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    float_image_batch = tf.cast(x_image_batch,tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    batch_size = tf.shape(x_image_batch)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    keep_prob = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    y_conv = max_net(float_image_batch,keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # compute softmax over raw predictions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    predictions = tf.nn.softmax(y_conv)[:,1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # restore variables from sleep123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    saver = tf.train.Saver()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    saver.restore(sess,FLAGS.saved_session)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    coord = tf.train.Coordinator()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    threads = tf.train.start_queue_runners(sess = sess,coord=coord)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create a variable to store all predicitons123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    all_predictios = store_predictions()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    batch_num = 0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print "start eval..." 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    while True or not coord.should_stop():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        batch_shape,test_ligand,test_predictions = sess.run([batch_size,ligand_file_name,predictions],feed_dict={keep_prob:1})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        all_predictios.add_batch(test_ligand,test_predictions)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        batch_num +=1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "batch num",batch_num,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "\tbatch size",batch_shape[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    all_predictios.save()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFevaluate_on_train_set()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFprint "All Done"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF