import time,sys,os123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#import av4_input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#import av4_networks123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#from av4_utils import utils,geom_utils123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#sys.path.append("../")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFsys.path.append(os.path.join(os.path.dirname(__file__), "../.."))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport affinity as af123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom config import FLAGS123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport cost_functions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass SamplingAgent:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """ Sampling Agent takes a single protein with a bound ligand and:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    1) samples many possible protein-ligand poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    2) samples many many camera views of the correct position123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    3) Outputs a single batch of images for which the gradient is highest.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: variance option for positives and negatives123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: clustering123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: add VDW possibilities123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: RMSD + the fraction of native contacts123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: add fast reject for overlapping atoms123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: in the future be sure not to find poses for a similar ligand123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: leave a single binding site on the level of data preparation123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: create fast reject for overlapping atoms of the protein and ligand123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # example 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # take all (do not remove middle) dig a hole in the landscape123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # batch is a training example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # needs labels123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # example 2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # remove semi-correct conformations123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # batch is a training example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #       # example 3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #       # remove semi-correct conformations123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #       # take many images of positive123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #       # image is a training example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def __init__(self,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 agent_name,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 gpu_name,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 training_q,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 logger,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 side_pixels=FLAGS.side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 pixel_size=FLAGS.pixel_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 batch_size=FLAGS.batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 num_threads=FLAGS.num_threads,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 shift_ranges = FLAGS.shift_ranges,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 shift_deltas = FLAGS.shift_deltas,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 initial_pose_evals = FLAGS.initial_pose_evals,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 train_b_init_poses = FLAGS.train_batch_init_poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 train_b_gen_poses = FLAGS.train_batch_gen_poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 sess = FLAGS.main_session):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # general parameters of the agent123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.agent_name = agent_name123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess = sess123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord = tf.train.Coordinator()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._batch_size = batch_size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._train_b_init_poses = train_b_init_poses123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._train_b_gen_poses = train_b_gen_poses123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._logger = logger123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Set up affine transform queue with the ligand positions.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_pose_tforms = tf.concat([af.geom.gen_identity_tform(initial_pose_evals),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                     af.geom.gen_spaced_affine_tform(shift_ranges,shift_deltas)],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                    0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._num_images_per_cycle = self.sess.run(tf.shape(lig_pose_tforms))[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        affine_tforms_q = tf.FIFOQueue(capacity=self._num_images_per_cycle, dtypes=tf.float32, shapes=[4, 4])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._affine_tforms_q_helpstop = affine_tforms_q.enqueue_many([af.geom.gen_identity_tform(num_threads*3)])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._affine_tforms_q_enq = affine_tforms_q.enqueue_many(lig_pose_tforms)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Create assign options for receptor and ligand.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_elem = tf.Variable([0],trainable=False, validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_coord = tf.Variable([[0.0,0.0,0.0]], trainable=False, validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_elem = tf.Variable([0],trainable=False, validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_coord = tf.Variable([[0.0,0.0,0.0]], trainable=False, validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._tform = affine_tforms_q.dequeue()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tformed_lig_coord,lig_pose_tform = af.geom.affine_tform(lig_coord, self._tform)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._lig_elem_plc = tf.placeholder(tf.int32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._lig_coord_plc = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._rec_elem_plc = tf.placeholder(tf.int32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._rec_coord_plc = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._ass_lig_elem = tf.assign(lig_elem, self._lig_elem_plc, validate_shape=False, use_locking=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._ass_lig_coord = tf.assign(lig_coord, self._lig_coord_plc, validate_shape=False, use_locking=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._ass_rec_elem = tf.assign(rec_elem, self._rec_elem_plc, validate_shape=False, use_locking=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._ass_rec_coord = tf.assign(rec_coord, self._rec_coord_plc, validate_shape=False, use_locking=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._lig_elem_shape = tf.shape(lig_elem)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._lig_coord_shape = tf.shape(lig_coord)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._rec_elem_shape = tf.shape(rec_elem)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._rec_coord_shape = tf.shape(rec_coord)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Create batches of images from coordinates and elements.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        complex_image,_,cameraview = af.input.complex_coords_to_image(lig_elem,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       tformed_lig_coord,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       rec_elem,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       rec_coord,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       pixel_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # calculate Root Mean Square Deviation for atoms of the transformed molecule compared to the initial one123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_RMSD = tf.reduce_mean(tf.square(tformed_lig_coord - lig_coord))**0.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # create and enqueue images in many threads, and deque and score images in a main thread123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        image_q = tf.FIFOQueue(capacity=train_b_init_poses + train_b_gen_poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               dtypes=[tf.float32,tf.float32,tf.float32,tf.float32],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               shapes=[[side_pixels,side_pixels,side_pixels], [4,4], [4,4], []])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._image_q_deq = image_q.dequeue()[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._image_q_enq = image_q.enqueue([complex_image, lig_pose_tform, cameraview, lig_RMSD])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._q_runner = af.utils.QueueRunner(image_q, [self._image_q_enq] * num_threads)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._image_b, self._lig_pose_tform_b, self._cameraview_b, self._lig_RMSD_b = image_q.dequeue_many(batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Evaluate images with the network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._keep_prob = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if gpu_name is not None:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            with tf.device(gpu_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                #with tf.variable_scope("network"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                y_conv = FLAGS.net.compute_output(self._image_b, self._keep_prob, batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # calculate both predictions, and costs for every ligand position in the batch123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._pred_b = tf.nn.softmax(y_conv)[:,1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._cost_b = cost_functions.cross_entropy_with_RMSD(y_conv, self._lig_RMSD_b)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Regeneration of the images from affine transform matrices for training.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # This part of the pipeline is for re-creation of already scored images from affine transform matrices123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # describing the conformation of the ligand, and a particular cameraview from which the 3D snapshot was taken.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Storing affine transform matrices instead of images allows to save memory while agent is running.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        r_tforms_cameraviews_q = tf.FIFOQueue(capacity=train_b_init_poses + train_b_gen_poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                              dtypes=[tf.float32,tf.float32],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                              shapes=[[4,4],[4,4]])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        r_image_q = tf.FIFOQueue(capacity=train_b_init_poses + train_b_gen_poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                 dtypes=[tf.float32,tf.float32],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                 shapes=[[side_pixels,side_pixels,side_pixels], []])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_image_b, self._r_lig_RMSD_b = r_image_q.dequeue_many(train_b_init_poses + train_b_gen_poses)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # create an image regenerator pipeline123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tforms_and_cameraviews_q_helpstop = r_tforms_cameraviews_q.enqueue_many(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            [af.geom.gen_identity_tform(num_threads*3), af.geom.gen_identity_tform(num_threads*3)])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tforms_plc = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_cameraviews_plc = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tforms_cameraviews_enq = r_tforms_cameraviews_q.enqueue_many([self._r_tforms_plc,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                              self._r_cameraviews_plc])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tform, self._r_cameraview = r_tforms_cameraviews_q.dequeue()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tformed_lig_coords, _ = af.geom.affine_tform(lig_coord, self._r_tform)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_complex_image,_ ,_ = af.input.complex_coords_to_image(lig_elem,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       self._r_tformed_lig_coords,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       rec_elem,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       rec_coord,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       pixel_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       self._r_cameraview)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # put images back to the image queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_lig_RMSD = tf.reduce_mean(tf.square(self._r_tformed_lig_coords - lig_coord))**0.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        r_image_q_enq = r_image_q.enqueue([self._r_complex_image, self._r_lig_RMSD])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_q_runner = af.utils.QueueRunner(r_image_q, [r_image_q_enq] * num_threads)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # dequeue image batch from regenerator, and enque it to the training queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.pass_batch_to_the_training_q = training_q.enqueue_many([self._r_image_b, self._r_lig_RMSD_b])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def grid_evaluate_positions(self, my_lig_elem, my_lig_coord, my_rec_elem, my_rec_coord):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """ Puts ligand in the center of every square of the box around the ligand, performs network evaluation of123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        every conformation.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        Raises: exception when number of positions evaluated in cycle is smaller than the batch size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Assign coordinates and elements of the ligand and protein. Add transformation matrices to the queue.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # enqueue all of the transformations for evaluation cycle123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run([self._affine_tforms_q_enq])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # assign elements and coordinates of protein and ligand;123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # shape of the variable will change from ligand to ligand123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run([self._ass_lig_elem,self._ass_lig_coord, self._ass_rec_elem,self._ass_rec_coord],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                      feed_dict={self._lig_elem_plc:my_lig_elem, self._lig_coord_plc:my_lig_coord,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                 self._rec_elem_plc:my_rec_elem, self._rec_coord_plc:my_rec_coord})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # re-initialize the evalutions class123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        evaluated = self.EvaluationsContainer(self._logger)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._logger.info("shapes of the ligand and protein:" + str(self.sess.run([self._lig_elem_shape,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                   self._lig_coord_shape,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                   self._rec_elem_shape,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                   self._rec_coord_shape])))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Evaluate all of the images of ligand and protein complexes in batches.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # start threads to fill the queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.enq_thr = self._q_runner.create_threads(self.sess, self._logger, self.coord, start=True, daemon=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # evaluate images in batches123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        num_batches_per_cycle = self._num_images_per_cycle // self._batch_size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if num_batches_per_cycle <= 0:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            raise Exception('number of grid points to evaluate too small for the batch size')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for i in range(num_batches_per_cycle):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            start = time.time()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            my_pred_b, my_cost_b, my_image_b, my_lig_pose_tform_b, my_cameraview_b, my_lig_RMSD_b = self.sess.run(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                [self._pred_b, self._cost_b, self._image_b, self._lig_pose_tform_b, self._cameraview_b, self._lig_RMSD_b],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                feed_dict = {self._keep_prob:1})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # save the predictions and cameraviews from the batch into evaluations container123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            lig_poses_evaluated = evaluated.add_batch(my_pred_b,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                      my_cost_b,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                      my_lig_pose_tform_b,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                      my_cameraview_b,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                      my_lig_RMSD_b)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # write run information to the log file123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info("\tligand_atoms:" + str(np.sum(np.array(my_image_b >7, dtype=np.int32))))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info("\tpositions evaluated:" + str(lig_poses_evaluated))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info("\texamples per second:" + str("%.2f" % (self._batch_size / (time.time() - start))) + "\n")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Select positively and negatively labeled images with the highest cost, and enqueue them into training queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        sel_lig_tforms, sel_cameraviews = evaluated.get_training_batch(cameraviews_initial_pose=self._train_b_init_poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                       generated_poses=self._train_b_gen_poses)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # accurately terminate all threads without closing the queue (uses custom QueueRunner class)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.request_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(self._affine_tforms_q_helpstop)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.join()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.clear_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        af.utils.dequeue_all(self.sess, self._tform, self._logger, "affine_tforms_queue")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        af.utils.dequeue_all(self.sess, self._image_q_deq, self._logger, "image_queue")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # regenerate a selected batch of images using the same (empty) image queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # enqueue the regenerator123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run([self._r_tforms_cameraviews_enq],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                      feed_dict={self._r_tforms_plc: sel_lig_tforms, self._r_cameraviews_plc: sel_cameraviews})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # start threads to fill the regenerator queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_enq_thr = self._r_q_runner.create_threads(self.sess, self._logger, coord=self.coord, start=True, daemon=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(self.pass_batch_to_the_training_q)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.request_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # accurately terminate all threads without closing the queue (using custom QueueRunner class)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(self._r_tforms_and_cameraviews_q_helpstop)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.join()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.clear_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        af.utils.dequeue_all(self.sess, self._r_tform, self._logger, "_r_tforms_and_cameraviews_queue")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        af.utils.dequeue_all(self.sess, self._image_q_deq, self._logger, "image_queue")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return None123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    class EvaluationsContainer:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """" Groups together information about the evaluated positions in a form of affine transform matrices. Reduces123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        all of the evaluated poses into training batch.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # TODO: add a possibility to generate new matrices based on performed evaluations.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # (aka genetic search in AutoDock)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def __init__(self, logger):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger = logger123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.preds = np.array([])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.costs = np.array([])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_pose_transforms = np.array([]).reshape([0, 4, 4])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.cameraviews = np.array([]).reshape([0, 4, 4])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_RMSDs = np.array([])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def add_batch(self, pred_batch, cost_batch, lig_pose_transform_batch, cameraview_batch, lig_RMSD_batch):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """ Adds batch of predictions for different positions and cameraviews of the ligand.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.preds = np.append(self.preds, pred_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.costs = np.append(self.costs, cost_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_pose_transforms = np.append(self.lig_pose_transforms, lig_pose_transform_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.cameraviews = np.append(self.cameraviews, cameraview_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_RMSDs = np.append(self.lig_RMSDs, lig_RMSD_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return len(self.preds)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def get_training_batch(self, cameraviews_initial_pose, generated_poses):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """ Returns examples with a highest cost/gradient.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            [cameraviews_initial_pose] + [generated_poses] should be == to [desired batch size] for training123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Raises: exception when the number of images requested for the training batch is higher then the number of123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            evaluated images in container.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if not (len(self.preds) == len(self.costs) == len(self.lig_pose_transforms) == len(self.cameraviews)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                        == len(self.lig_RMSDs)):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                raise Exception('Number of records for different categories in EvauationsContainer differs.')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # order all parameters by costs in ascending order123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            order = np.argsort(self.costs)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.preds = self.preds[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.costs = self.costs[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_pose_transforms = self.lig_pose_transforms[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.cameraviews = self.cameraviews[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_RMSDs = self.lig_RMSDs[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # take examples for which the cost is highest123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            init_poses_idx = (np.where(self.lig_RMSDs < 0.01)[0])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            gen_poses_idx = (np.where(self.lig_RMSDs > 0.01)[0])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if len(init_poses_idx) < cameraviews_initial_pose:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                raise Exception('Number of ligand initial poses requested is more than EvaluationsContainer has')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if len(gen_poses_idx) < generated_poses:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                raise Exception('Number of ligand generated poses requested is more than EvaluationsContainer has')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            sel_init_poses_idx = init_poses_idx[-cameraviews_initial_pose:]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            sel_gen_poses_idx = gen_poses_idx[-generated_poses:]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # print a lot of statistics for debugging/monitoring purposes123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info("statistics sampled conformations:")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            var_list = {'lig_RMSDs':self.lig_RMSDs,'preds':self.preds,'costs':self.costs}123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info(af.utils.var_stats(var_list))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info("statistics for initial conformations")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            var_list = {'lig_RMSDs':self.lig_RMSDs[init_poses_idx], 'preds':self.preds[init_poses_idx],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                        'costs':self.costs[init_poses_idx]}123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info(af.utils.var_stats(var_list))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info("statistics for generated conformations")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            var_list = {'lig_RMSDs':self.lig_RMSDs[gen_poses_idx], 'preds':self.preds[gen_poses_idx],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                        'costs':self.costs[gen_poses_idx]}123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info(af.utils.var_stats(var_list))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info("statistics for selected (hardest) initial conformations")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            var_list = {'lig_RMSDs':self.lig_RMSDs[sel_init_poses_idx], 'preds':self.preds[sel_init_poses_idx],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                        'costs':self.costs[sel_init_poses_idx]}123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info(af.utils.var_stats(var_list))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info("statistics for selected (hardest) generated conformations")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            var_list = {'lig_RMSDs':self.lig_RMSDs[sel_gen_poses_idx], 'preds':self.preds[sel_gen_poses_idx],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                        'costs':self.costs[sel_gen_poses_idx]}123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self._logger.info(af.utils.var_stats(var_list))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            sel_idx = np.hstack([sel_init_poses_idx, sel_gen_poses_idx])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return self.lig_pose_transforms[sel_idx], self.cameraviews[sel_idx]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF