#!/usr/bin/env python123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# -*- coding: utf-8 -*-123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFMain file for js_train123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport net123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom utils import *123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef train():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Make logging very verbose123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.logging.set_verbosity(tf.logging.DEBUG)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # with tf.Session(config=tf.ConfigProto(log_device_placement=False,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #                                       operation_timeout_in_ms=600000)) as sess:  # Stop after 10 minutes123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.Session() as sess:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Get images and labels for MSHAPES123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Setting up getting batches and labels")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        filequeue, images_batch, labels_batch = net.inputs(eval_data=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Got two batches")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Image batch shape: ")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print(images_batch.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Labels batch shape:")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print(labels_batch.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Create a global step variable.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # This is sometimes needed for training123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # (e.g. variable weight decay)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        global_step = tf.contrib.framework.get_or_create_global_step()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Build a Graph that computes the logits predictions from the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # inference model.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        keep_prob = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #  ████████╗███████╗███████╗████████╗██╗███╗   ██╗ ██████╗     ███████╗███████╗████████╗██╗   ██╗██████╗123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #  ╚══██╔══╝██╔════╝██╔════╝╚══██╔══╝██║████╗  ██║██╔════╝     ██╔════╝██╔════╝╚══██╔══╝██║   ██║██╔══██╗123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #     ██║   █████╗  ███████╗   ██║   ██║██╔██╗ ██║██║  ███╗    ███████╗█████╗     ██║   ██║   ██║██████╔╝123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #     ██║   ██╔══╝  ╚════██║   ██║   ██║██║╚██╗██║██║   ██║    ╚════██║██╔══╝     ██║   ██║   ██║██╔═══╝123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #     ██║   ███████╗███████║   ██║   ██║██║ ╚████║╚██████╔╝    ███████║███████╗   ██║   ╚██████╔╝██║123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #     ╚═╝   ╚══════╝╚══════╝   ╚═╝   ╚═╝╚═╝  ╚═══╝ ╚═════╝     ╚══════╝╚══════╝   ╚═╝    ╚═════╝ ╚═╝123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #   Here, we have a large conditional statement that, given the command-line `training type` parameter,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #   builds the corresponding neural network architecture. However, some of the architectures return123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #   just the logits; other architectures return other information as well, such as area cost and s-cost.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #   Thus, there are arrays, such as `area_cost_runs`, which store a list of training types that return123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #   information other than logits, so that those variables can be initialized and accessed properly.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #   There is probably a much better way of running a large number of architecture tests than the basically123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #   hardcoded way it's done now--however, I don't have any better ideas for a testing suite that still have123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #   the desireable properties that this suite has, such as being able to simply write123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #        ```sbatch run.sh 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #           sbatch run.sh 2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #           sbatch run.sh 3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #                  ...     ```123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #   on XStream and run all the tests with a simple bash for loop.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # logits = net.siamese_net(images_batch, num_features=2000)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        logits = net.op_stm_net(images_batch)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Calculate loss.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        loss = net.loss(logits, labels_batch)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Build a Graph that trains the model with one batch of examples and123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # updates the model parameters.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        train_op = net.train(loss)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Merge the summaries and write them to the summaries directory123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        merged_summaries = tf.summary.merge_all()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        train_writer = tf.summary.FileWriter(FLAGS.SUMMARIES_DIR + '/train', sess.graph)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        test_writer = tf.summary.FileWriter(FLAGS.SUMMARIES_DIR + '/test')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Set up session saver/loader123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        saver = tf.train.Saver(var_list=(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Restore from session, if necessary123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if FLAGS.RESTORE:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print("Restoring...")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            saver = tf.train.Saver()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            saver.restore(sess, FLAGS.RESTORE_FROM)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print("Restored.")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # If not restoring variables, initialize them123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print("Initializing global variables")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # tf.set_random_seed(42)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            tf.global_variables_initializer().run()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print("Finished")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Start the training coorinator and the queue runners123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Starting coordinator and queue runners")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        coord = tf.train.Coordinator()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        threads = tf.train.start_queue_runners(coord=coord, sess=sess)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Ok")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # tf.group(enqueues, reenqueues)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # enqueue everything as needed123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # e = sess.run([enqueues])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # print("Enqueue result ")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # print(e)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # queue_size = sess.run(filequeue.size())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # print("Initial queue size: " + str(queue_size))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Run training for a certain number of steps123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for i in xrange(0, FLAGS.MAX_TRAINING_STEPS):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Run the training operaion, and get the loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            _, my_loss = sess.run([train_op, loss])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Do some formatting magic on the loss by storing it as an array123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            ml = np.array(my_loss)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Report the step and the cross-entropy loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print('Step: %d     Cross entropy loss: % 6.2f' % (i, ml))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Save the variable summaries to disk123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # summary, _ = sess.run([merged_summaries, i])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # train_writer.add_summary(summary, i)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Send the cross-entropy to the knock-off tensorboard every ~50 steps123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if i % FLAGS.SEND_LOSS_TO_POOR_MANS_TENSORBOARD_EVERY_N_STEPS == 0:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                if FLAGS.INTERNET:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    send_cross_entropy_to_poor_mans_tensorboard(str(ml))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                with open("cel_log_.csv", "a") as myfile:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    myfile.write(str(i) + "," + str(ml) + "\n")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Every ~1000 steps, save the results and send an email123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if i % FLAGS.SAVE_NETSTATE_AND_EMAIL_STATUS_EVERY_N_STEPS == 0 and i != 0:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                if FLAGS.INTERNET:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    # Send informative email via νe123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    notify("Current cross-entropy loss: " + str(ml) + ".",123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                           subject="Running stats [step " + str(i) + "]",123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                           email=FLAGS.NOTIFICATION_EMAIL)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                # Save the current net state123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                saver.save(sess, "summaries/netstate/saved_state" + FLAGS.RUN_NAME, global_step=i)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # If the loss is not a number, stop training and send an email123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if np.isnan(ml):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                print("Oops")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                notify("Diverged :(", subject="Process ended")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                sys.exit(0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Finished training")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Finish off the filename queue coordinator.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Requesting thread stop")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        coord.request_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Ok")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Joining threads")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        coord.join(threads)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Ok")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Finished")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF