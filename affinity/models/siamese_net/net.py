#!/usr/bin/env python123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# -*- coding: utf-8 -*-123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFnet.py123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThis file contains the deep learning architectures for JSHAPES.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport os123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport re123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport sys123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom constants import FLAGS123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFsys.path.append(os.path.join(os.path.dirname(__file__), "../../../"))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom affinity.networks.networks_ops import weight_variable, bias_variable123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom affinity.utils import print_progress_bar123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom affinity.utils.utils_ops import die123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# Global constants describing the MSHAPES data set.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIMAGE_SIZE = FLAGS.IMAGE_SIZE123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFDIM = IMAGE_SIZE123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFNUM_CLASSES = FLAGS.NUM_CLASSES123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = FLAGS.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = FLAGS.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFTOWER_NAME = FLAGS.TOWER_NAME123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef inputs(eval_data):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Construct input for MSHAPES evaluation using the Reader ops.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Args:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      eval_data: bool, indicating if one should use the train or eval data set.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :returns:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      labels: Labels. 1D tensor of [batch_size] size.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Raises:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      ValueError: If no data_dir123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    if not FLAGS.DATA_DIR:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        raise ValueError('Please supply a data_dir')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    data_dir = os.path.join(FLAGS.DATA_DIR, '')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    (filequeue, (images, labels)) = input.inputs(eval_data=eval_data,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                 data_dir=data_dir,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                 batch_size=FLAGS.BATCH_SIZE)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # print("Reenqueues: ")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # print(reenqueues)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    if FLAGS.USE_FP16:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images = tf.cast(images, tf.float16)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        labels = tf.cast(labels, tf.float16)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return filequeue, images, labels123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef train(total_loss):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Trains the JSPAMES model.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Uses an AdamOptimizer to minimize the loss.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Args:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      total_loss: Total loss from loss().123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :returns:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      train_op: op for training.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('train'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        train_op = tf.train.AdamOptimizer(FLAGS.LEARNING_RATE).minimize(total_loss)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return train_op123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# def inference_pretrained(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#     this could also be the output a different Keras model or layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#     input_tensor = Input(shape=(150, 150, 3))  # this assumes K.image_data_format() == 'channels_last'123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#     model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#     return model123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef split(combined_images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    lock_images, key_images = tf.split(combined_images, num_or_size_splits=2, axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return lock_images, key_images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef get_area_loss_2(transformed_lock_images, transformed_key_images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Gets the area (overlap) loss between the lock images and the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    key images.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param transformed_lock_images: The lock images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                    [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param transformed_key_images: The key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                    [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: The scaled area loss, (in the range [0, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    difference = tf.subtract(transformed_lock_images, transformed_key_images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    difference = tf.multiply(difference, 1.0 / 255.0)  # Change [0, 255] to [0, 1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print(difference.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    abs_difference = tf.abs(difference)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print(abs_difference.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    area_loss = tf.reduce_sum(tf.cast(abs_difference, tf.float32))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    area_loss = area_loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    dims = transformed_lock_images.get_shape().as_list()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    max_possible_loss = dims[0] * dims[1] * dims[2] * 2 + 3.0 - 3.0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print(max_possible_loss)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    scaled_area_loss = area_loss / max_possible_loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return scaled_area_loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef get_area_loss_1(combined_images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Gets the area (overlap) loss between the lock images and the key images.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param combined_images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                            [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: The scaled area loss, (in the range [0, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    lock_images, key_images = tf.split(combined_images, num_or_size_splits=2, axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return get_area_loss_2(lock_images, key_images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#   █████╗ ██████╗  ██████╗██╗  ██╗██╗████████╗███████╗ ██████╗████████╗██╗   ██╗██████╗ ███████╗███████╗123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#  ██╔══██╗██╔══██╗██╔════╝██║  ██║██║╚══██╔══╝██╔════╝██╔════╝╚══██╔══╝██║   ██║██╔══██╗██╔════╝██╔════╝123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#  ███████║██████╔╝██║     ███████║██║   ██║   █████╗  ██║        ██║   ██║   ██║██████╔╝█████╗  ███████╗123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#  ██╔══██║██╔══██╗██║     ██╔══██║██║   ██║   ██╔══╝  ██║        ██║   ██║   ██║██╔══██╗██╔══╝  ╚════██║123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#  ██║  ██║██║  ██║╚██████╗██║  ██║██║   ██║   ███████╗╚██████╗   ██║   ╚██████╔╝██║  ██║███████╗███████║123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#  ╚═╝  ╚═╝╚═╝  ╚═╝ ╚═════╝╚═╝  ╚═╝╚═╝   ╚═╝   ╚══════╝ ╚═════╝   ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚══════╝╚══════╝123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#   Current implemented architectures:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       1. Combined STM net123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       2. Combined STM net with area loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       3. Half STM net123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       4. Half STM net with area loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       5. General Half STM net123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       6. General Half STM net with area loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       7. General Half and Half STM net123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       8. General Half and Half STM net with area loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       9. General Half and Half STM net with summed area loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#      10. General Half and Half STM net with summed weighted area loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#   Helper functions:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       inference: runs a basic image-recognition network on the inputs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#                  (often used at the end of the STM implementations)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       specialized_stm_pair: a layer containing two all-seeing STMs,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#                             one transforming the key images, the other123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#                             transforming the lock images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       specialized_stm_pair_half: a layer containing one all-seeing STM,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#                                  which transforms the key images only.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef siamese_net(images_batch, num_features):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    lock_images, key_images = split(images_batch)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    lock_features_vector = feature_extract(lock_images, feature_num=num_features, name="lock_features")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    key_features_vector = feature_extract(key_images, feature_num=num_features, name="key_features")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Lock features vector shape: ")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print(lock_features_vector.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Key features vector shape: ")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print(key_features_vector.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Take the vectorwise dot product of the batched feature matrices123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    combined_features = tf.reduce_sum(tf.multiply(lock_features_vector, key_features_vector), 1, keep_dims=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Combined features shape: ")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print(combined_features.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    dense = tf.layers.dense(inputs=combined_features, units=1024, activation=tf.nn.relu)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    dense = tf.layers.dense(inputs=dense, units=1024, activation=tf.nn.relu)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    dense = tf.layers.dense(inputs=dense, units=1024, activation=tf.nn.relu)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    dense = tf.layers.dense(inputs=dense, units=1024, activation=tf.nn.relu)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    dense = tf.layers.dense(inputs=dense, units=1024, activation=tf.nn.relu)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    dense = tf.layers.dense(inputs=dense, units=1024, activation=tf.nn.relu)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    dense = tf.layers.dense(inputs=dense, units=192, activation=tf.nn.relu)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("dense shape: ")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print(dense.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # linear layer(WX + b),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # We don't apply softmax here because123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # and performs the softmax internally for efficiency.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.variable_scope('softmax_linear') as scope:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                              stddev=1 / 192.0, wd=0.0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        biases = _variable_on_cpu('biases', [NUM_CLASSES],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                  tf.constant_initializer(0.0))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        softmax_linear = tf.add(tf.matmul(dense, weights), biases, name=scope.name)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        _activation_summary(softmax_linear)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return softmax_linear123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef feature_extract(images, feature_num, name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope(name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        w1 = weight_variable([5, 5, 1, 64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        w2 = weight_variable([5, 5, 64, 64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        w3 = weight_variable([5, 5, 64, 64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        w4 = weight_variable([5, 5, 64, 64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        w5 = weight_variable([5, 5, 64, 64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b1 = bias_variable([64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b2 = bias_variable([64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b3 = bias_variable([64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b4 = bias_variable([64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b5 = bias_variable([64])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        fc1w = weight_variable([7500, 1024])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        fc1b = bias_variable([1024])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        fc2w = weight_variable([1024, 256])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        fc2b = bias_variable([256])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        fc3w = weight_variable([256, 2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        fc3b = bias_variable([2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv1 = tf.nn.conv2d(images, w1, strides=[1, 1, 1, 1], padding='SAME') + b1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu1 = tf.nn.relu(h_conv1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool1 = tf.nn.max_pool(h_relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv2 = tf.nn.conv2d(h_pool1, w2, strides=[1, 1, 1, 1], padding='SAME') + b2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu2 = tf.nn.relu(h_conv2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool2 = tf.nn.max_pool(h_relu2, ksize=[1, 3, 3, 1], strides=[1, 3, 3, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv3 = tf.nn.conv2d(h_pool2, w3, strides=[1, 1, 1, 1], padding='SAME') + b3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu3 = tf.nn.relu(h_conv3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool3 = tf.nn.max_pool(h_relu3, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv4 = tf.nn.conv2d(h_pool3, w4, strides=[1, 1, 1, 1], padding='SAME') + b4123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu4 = tf.nn.relu(h_conv4)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool4 = tf.nn.max_pool(h_relu4, ksize=[1, 5, 5, 1], strides=[1, 5, 5, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv5 = tf.nn.conv2d(h_pool4, w5, strides=[1, 1, 1, 1], padding='SAME') + b5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu5 = tf.nn.relu(h_conv5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool5 = tf.nn.max_pool(h_relu5, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("h_pool5 shape:")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print(h_pool5.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # local3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope(name + 'local3') as scope:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Move everything into depth so we can perform a single matrix multiply.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            reshape = tf.reshape(h_pool5, [FLAGS.BATCH_SIZE, -1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            dim = reshape.get_shape()[1].value123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            weights = _variable_with_weight_decay('weights', shape=[dim, feature_num],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                  stddev=0.04, wd=0.004)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            biases = _variable_on_cpu('biases', [feature_num], tf.constant_initializer(0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            _activation_summary(local3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("local3 shape:")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print(local3.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return local3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef combined_stm_net(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Combined STM net--both the lock images and the key images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    are transformed individually, by their own STMs.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Running Combined STM Net.")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    combined_transformed_images = specialized_stm_pair(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Now, we can run a normal convolutional neural123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # network on the images, since what we output123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # is the same exact shape as the input,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # except with two spatial transform modules123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # now inside of it.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return inference(combined_transformed_images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef combined_stm_net_with_area_loss(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Combined STM net with area loss.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Area loss and logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Running Combined STM Net with area loss.")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    combined_transformed_images = specialized_stm_pair(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    area_loss = get_area_loss_1(combined_transformed_images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Now, we can run a normal convolutional neural123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # network on the images, since what we output123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # is the same exact shape as the input,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # except with two spatial transform modules123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # now inside of it.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return area_loss, inference(combined_transformed_images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef half_stm_net(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Half STM net--STM net where only the keys123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    are transformed (but the STM module can see123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    both the lock and the key images).123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef half_stm_net_with_area_cost(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Half STM net with area cost.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Area cost and logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    area_cost = get_area_loss_1(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return area_cost, inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef general_half_stm_net(images, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    General half STM net--a half STM net with123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    `num_stm_layers` STM modules.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param num_stm_layers: The number of STM layers in the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                           network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    for i in xrange(1, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef general_half_stm_net_with_area_cost(images, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    General half STM net with area cost.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param num_stm_layers: The number of STM layers in the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                           network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Area cost and logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    for i in xrange(1, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    area_cost = get_area_loss_1(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return area_cost, inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef general_half_and_half_stm_net(images, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    General Half and Half STM net.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    A network where both the lock images and the key images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    are individually transformed through `num_stm_layers`123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    STM layers each.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param num_stm_layers: The number of STM layers in the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                           network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    for i in xrange(1, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images = specialized_stm_pair(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef general_half_and_half_stm_net_with_area_cost(images, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    General Half and Half STM net with area cost.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param num_stm_layers: The number of STM layers in the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                           network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Area cost, logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    for i in xrange(1, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images = specialized_stm_pair(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    area_cost = get_area_loss_1(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return area_cost, inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef general_half_and_half_stm_net_with_summed_area_cost(images, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    General Half and Half STM net with summed area cost.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    (That is, the area cost is the sum of the area costs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    after each individual spatial transform module).123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param num_stm_layers: The number of STM layers in the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                           network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Area cost, logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    area_cost = 0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    for i in xrange(1, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images = specialized_stm_pair(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        area_cost = area_cost + get_area_loss_1(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return area_cost, inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef general_half_and_half_stm_net_with_weighted_summed_area_cost(images, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        General Half and Half STM net with weighted summed area cost.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        (That is, the area cost is the weighted sum of the area costs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        after each individual spatial transform module).123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :param images: The combined lock and key images, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                       [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :param num_stm_layers: The number of STM layers in the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :return: Area cost, logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    area_cost = 0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    for i in xrange(1, num_stm_layers):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images = specialized_stm_pair(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        area_cost = area_cost + get_area_loss_1(images) ** i123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return area_cost, inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF@DeprecationWarning123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef op_stm_net(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    An STM net with six layers of affine transform123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    applied to the key half of the lock-key123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    batch.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Will soon be deprecated in favor of123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    `general_half_and_half_stm_net`, which123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    is a more general version of this network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock/key images,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   with shape `[BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]`.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: Logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Six layers of affine transform123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF@DeprecationWarning123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef op_stm_net_with_area_cost(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        An STM net with six layers of affine transform123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        applied to the key half of the lock-key123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        batch, along with final area cost.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        Will soon be deprecated in favor of123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        `general_half_and_half_stm_net_with_area_cost`123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        which is a more general version of this network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :param images: The combined lock/key images,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                       with shape `[BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]`.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :return: Logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Six layers of affine transform123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    area_cost = get_area_loss_1(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return area_cost, inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef specialized_stm_pair(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    A specialized STM pair layer.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Takes in combined lock/key images of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2], and trains123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    a spatial transformer network on the full images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    that learns two (affine) transformations123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    separately; one for the lock images, and one for123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    the key images.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock/key images,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   of size [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: A duplet of the transformed lock images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF             and the transformed key images, each of123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF             size [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('specialized_stm_pair'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.name_scope('h_fc1'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Set up an LTM after six fully-connected layers123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            out_size = tf.convert_to_tensor([IMAGE_SIZE, IMAGE_SIZE])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            n_fc = 6123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            W_fc1 = tf.Variable(tf.zeros([1200 * 1600 * 3, n_fc]), name='W_fc1')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            initial = np.array([[0.5, 0, 0], [0, 0.5, 0]])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            initial = initial.astype('float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            initial = initial.flatten()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            b_fc1 = tf.Variable(initial_value=initial, name='b_fc1')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            h_fc1 = tf.matmul(tf.zeros([FLAGS.BATCH_SIZE, 1200 * 1600 * 3]), W_fc1) + b_fc1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            tf.summary.histogram('h_fc1', h_fc1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Now, we train two specialized transformers on the data.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # What the specialized transformers do, is they see123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # the entire combined image (specifically, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # [BATCH_SIZE, 100, 100, 2]). However, the transform123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # they learn is only applied to one of the two123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # size [BATCH_SIZE, 100, 100, 1] layers--that is,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # either just the lock images batch, or just the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # key images batch.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # We have two specialized transformer to123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # accommodate both the lock images and the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # key images.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        transformed_key_images = specialized_transformer(images, h_fc1, out_size, level_to_transform=1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        transformed_key_images = tf.reshape(transformed_key_images, [FLAGS.BATCH_SIZE, DIM, DIM, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        transformed_lock_images = specialized_transformer(images, h_fc1, out_size, level_to_transform=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        transformed_lock_images = tf.reshape(transformed_lock_images, [FLAGS.BATCH_SIZE, DIM, DIM, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        combined_transformed_images = tf.stack([transformed_lock_images, transformed_key_images], axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        combined_transformed_images = tf.squeeze(combined_transformed_images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return combined_transformed_images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef specialized_stm_pair_half(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    A specialized STM pair layer.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Takes in combined lock/key images of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2], and trains123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    a spatial transformer network on the full images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    that learns one (affine) transformationfor the key123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images, while leaving the lock images intact.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock/key images,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   of size [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: A duplet of the transformed lock images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF             and the transformed key images, each of123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF             size [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Set up an LTM after six fully-connected layers123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    out_size = tf.convert_to_tensor([IMAGE_SIZE, IMAGE_SIZE])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    n_fc = 6123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    W_fc1 = tf.Variable(tf.zeros([1200 * 1600 * 3, n_fc]), name='W_fc1')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = np.array([[0.5, 0, 0], [0, 0.5, 0]])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = initial.astype('float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = initial.flatten()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    b_fc1 = tf.Variable(initial_value=initial, name='b_fc1')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1 = tf.matmul(tf.zeros([FLAGS.BATCH_SIZE, 1200 * 1600 * 3]), W_fc1) + b_fc1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Now, we train two specialized transformers on the data.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # What the specialized transformers do, is they see123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # the entire combined image (specifically, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # [BATCH_SIZE, 100, 100, 2]). However, the transform123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # they learn is only applied to one of the two123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # size [BATCH_SIZE, 100, 100, 1] layers--that is,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # either just the lock images batch, or just the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # key images batch.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # We have two specialized transformer to123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # accommodate both the lock images and the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # key images.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    transformed_key_images = specialized_transformer(images, h_fc1, out_size, level_to_transform=1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    transformed_key_images = tf.reshape(transformed_key_images, [FLAGS.BATCH_SIZE, DIM, DIM, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # transformed_lock_images = specialized_transformer(images, h_fc1, out_size, level_to_transform=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # transformed_lock_images = tf.reshape(transformed_lock_images, [FLAGS.BATCH_SIZE, DIM, DIM, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    untransformed_lock_images, _ = tf.split(images, num_or_size_splits=2, axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    combined_transformed_images = tf.stack([untransformed_lock_images, transformed_key_images], axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    combined_transformed_images = tf.squeeze(combined_transformed_images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return combined_transformed_images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef specialized_stm_pair_half_with_level(images, level_to_transform):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    A specialized STM pair layer.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Takes in combined lock/key images of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2], and trains123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    a spatial transformer network on the full images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    that learns one (affine) transformationfor the key123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images, while leaving the lock images intact.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param images: The combined lock/key images,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   of size [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param level_to_transform: Which level of the tensor to transform.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               Accepts values in range [0, 1].123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: A duplet of the transformed lock images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF             and the transformed key images, each of123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF             size [BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Set up an LTM after six fully-connected layers123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    out_size = tf.convert_to_tensor([IMAGE_SIZE, IMAGE_SIZE])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    n_fc = 6123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    W_fc1 = tf.Variable(tf.zeros([1200 * 1600 * 3, n_fc]), name='W_fc1')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = np.array([[0.5, 0, 0], [0, 0.5, 0]])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = initial.astype('float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = initial.flatten()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    b_fc1 = tf.Variable(initial_value=initial, name='b_fc1')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1 = tf.matmul(tf.zeros([FLAGS.BATCH_SIZE, 1200 * 1600 * 3]), W_fc1) + b_fc1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Now, we train two specialized transformers on the data.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # What the specialized transformers do, is they see123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # the entire combined image (specifically, of size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # [BATCH_SIZE, 100, 100, 2]). However, the transform123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # they learn is only applied to one of the two123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # size [BATCH_SIZE, 100, 100, 1] layers--that is,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # either just the lock images batch, or just the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # key images batch.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # We have two specialized transformer to123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # accommodate both the lock images and the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # key images.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    transformed_images = specialized_transformer(images, h_fc1, out_size, level_to_transform=level_to_transform)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    transformed_images = tf.reshape(transformed_images, [FLAGS.BATCH_SIZE, DIM, DIM, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # transformed_lock_images = specialized_transformer(images, h_fc1, out_size, level_to_transform=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # transformed_lock_images = tf.reshape(transformed_lock_images, [FLAGS.BATCH_SIZE, DIM, DIM, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    untransformed_images = 0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    if level_to_transform == 0:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        _, untransformed_images = tf.split(images, num_or_size_splits=2, axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    elif level_to_transform == 1:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        untransformed_images, _ = tf.split(images, num_or_size_splits=2, axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    combined_transformed_images = tf.stack([untransformed_images, transformed_images], axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    combined_transformed_images = tf.squeeze(combined_transformed_images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return combined_transformed_images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef inference(images, layer_name='inference'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Build the JSHAPES model.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Args:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      images: Images returned from distorted_inputs() or inputs().123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :returns:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      Logits.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope(layer_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # We instantiate all variables using tf.get_variable() instead of123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # tf.Variable() in order to share variables across multiple GPU training runs.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # If we only ran this model on a single GPU, we could simplify this function123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # by replacing all instances of tf.get_variable() with tf.Variable().123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # conv1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('conv1') as scope:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            kernel = _variable_with_weight_decay('weights',123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                 shape=[5, 5, 2, 64],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                 stddev=5e-3,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                 wd=0.0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            biases = _variable_on_cpu('biases', [64], tf.constant_initializer(1e-2))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            pre_activation = tf.nn.bias_add(conv, biases)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            conv1 = tf.nn.relu(pre_activation, name=scope.name)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            _activation_summary(conv1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # pool1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               padding='SAME', name='pool1')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # norm1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                          name='norm1')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # conv2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('conv2') as scope:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            kernel = _variable_with_weight_decay('weights',123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                 shape=[5, 5, 64, 64],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                 stddev=5e-2,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                 wd=0.0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            pre_activation = tf.nn.bias_add(conv, biases)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            conv2 = tf.nn.relu(pre_activation, name=scope.name)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            _activation_summary(conv2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # norm2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                          name='norm2')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # pool2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               strides=[1, 2, 2, 1], padding='SAME', name='pool2')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # local3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('local3') as scope:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Move everything into depth so we can perform a single matrix multiply.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            reshape = tf.reshape(pool2, [FLAGS.BATCH_SIZE, -1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            dim = reshape.get_shape()[1].value123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            weights = _variable_with_weight_decay('weights', shape=[dim, 384],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                  stddev=0.04, wd=0.004)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            _activation_summary(local3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # local4123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('local4') as scope:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            weights = _variable_with_weight_decay('weights', shape=[384, 192],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                  stddev=0.04, wd=0.004)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            _activation_summary(local4)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # linear layer(WX + b),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # We don't apply softmax here because123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # and performs the softmax internally for efficiency.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('softmax_linear') as scope:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                  stddev=1 / 192.0, wd=0.0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            biases = _variable_on_cpu('biases', [NUM_CLASSES],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                      tf.constant_initializer(0.0))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            _activation_summary(softmax_linear)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return softmax_linear123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef inference_with_stm(images):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half_with_level(images, level_to_transform=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    images = specialized_stm_pair_half_with_level(images, level_to_transform=1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return inference(images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef transformer(U, theta, out_size, name='SpatialTransformer', **kwargs):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Spatial Transformer Layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Implements a spatial transformer layer as described in [1]_.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Based on [2]_ and edited by David Dao for Tensorflow.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Parameters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ----------123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    U : float123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        The output of a convolutional net should have the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        shape [num_batch, height, width, num_channels].123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    theta: float123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        The output of the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        localisation network should be [num_batch, 6].123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    out_size: tuple of two ints123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        The size of the output of the network (height, width)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    References123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ----------123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    .. [1]  Spatial Transformer Networks123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Submitted on 5 Jun 2015123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Notes123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    -----123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    To initialize the network to the identity transform init123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ``theta`` to :123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        identity = np.array([[1., 0., 0.],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                             [0., 1., 0.]])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        identity = identity.flatten()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        theta = tf.Variable(initial_value=identity)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _repeat(x, n_repeats):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('_repeat'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            rep = tf.transpose(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                tf.expand_dims(tf.ones(shape=tf.stack([n_repeats, ])), 1), [1, 0])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            rep = tf.cast(rep, 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x = tf.matmul(tf.reshape(x, (-1, 1)), rep)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return tf.reshape(x, [-1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _interpolate(im, x, y, out_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('_interpolate'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # constants123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            num_batch = tf.shape(im)[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            height = tf.shape(im)[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            width = tf.shape(im)[2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            channels = tf.shape(im)[3]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x = tf.cast(x, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y = tf.cast(y, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            height_f = tf.cast(height, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            width_f = tf.cast(width, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            out_height = out_size[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            out_width = out_size[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            zero = tf.zeros([], dtype='int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            max_y = tf.cast(tf.shape(im)[1] - 1, 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            max_x = tf.cast(tf.shape(im)[2] - 1, 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # scale indices from [-1, 1] to [0, width/height]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x = (x + 1.0) * (width_f) / 2.0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y = (y + 1.0) * (height_f) / 2.0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # do sampling123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x0 = tf.cast(tf.floor(x), 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x1 = x0 + 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y0 = tf.cast(tf.floor(y), 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y1 = y0 + 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x0 = tf.clip_by_value(x0, zero, max_x)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x1 = tf.clip_by_value(x1, zero, max_x)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y0 = tf.clip_by_value(y0, zero, max_y)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y1 = tf.clip_by_value(y1, zero, max_y)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            dim2 = width123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            dim1 = width * height123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            base = _repeat(tf.range(num_batch) * dim1, out_height * out_width)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            base_y0 = base + y0 * dim2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            base_y1 = base + y1 * dim2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            idx_a = base_y0 + x0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            idx_b = base_y1 + x0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            idx_c = base_y0 + x1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            idx_d = base_y1 + x1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # use indices to lookup pixels in the flat image and restore123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # channels dim123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            im_flat = tf.reshape(im, tf.stack([-1, channels]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            im_flat = tf.cast(im_flat, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Ia = tf.gather(im_flat, idx_a)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Ib = tf.gather(im_flat, idx_b)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Ic = tf.gather(im_flat, idx_c)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Id = tf.gather(im_flat, idx_d)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # and finally calculate interpolated values123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x0_f = tf.cast(x0, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x1_f = tf.cast(x1, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y0_f = tf.cast(y0, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y1_f = tf.cast(y1, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            wa = tf.expand_dims(((x1_f - x) * (y1_f - y)), 1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            wb = tf.expand_dims(((x1_f - x) * (y - y0_f)), 1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            wc = tf.expand_dims(((x - x0_f) * (y1_f - y)), 1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            wd = tf.expand_dims(((x - x0_f) * (y - y0_f)), 1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            output = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return output123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _meshgrid(height, width):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('_meshgrid'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # This should be equivalent to:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #                         np.linspace(-1, 1, height))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #  ones = np.ones(np.prod(x_t.shape))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                            tf.transpose(tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                            tf.ones(shape=tf.stack([1, width])))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x_t_flat = tf.reshape(x_t, (1, -1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y_t_flat = tf.reshape(y_t, (1, -1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            ones = tf.ones_like(x_t_flat)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return grid123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _transform(theta, input_dim, out_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('_transform'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            num_batch = tf.shape(input_dim)[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            height = tf.shape(input_dim)[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            width = tf.shape(input_dim)[2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            num_channels = tf.shape(input_dim)[3]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            theta = tf.reshape(theta, (-1, 2, 3))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            theta = tf.cast(theta, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # grid of (x_t, y_t, 1), eq (1) in ref [1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            height_f = tf.cast(height, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            width_f = tf.cast(width, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            out_height = out_size[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            out_width = out_size[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = _meshgrid(out_height, out_width)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.expand_dims(grid, 0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.reshape(grid, [-1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.tile(grid, tf.stack([num_batch]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.reshape(grid, tf.stack([num_batch, 3, -1]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Transform A x (x_t, y_t, 1)^T -> (x_s, y_s)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            T_g = tf.matmul(theta, grid)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x_s = tf.slice(T_g, [0, 0, 0], [-1, 1, -1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y_s = tf.slice(T_g, [0, 1, 0], [-1, 1, -1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x_s_flat = tf.reshape(x_s, [-1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y_s_flat = tf.reshape(y_s, [-1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            input_transformed = _interpolate(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                input_dim, x_s_flat, y_s_flat,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                out_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            output = tf.reshape(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return output123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.variable_scope(name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        output = _transform(theta, U, out_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return output123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef specialized_transformer(U, theta, out_size, level_to_transform, name='SpatialTransformer', **kwargs):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Spatial Transformer Layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Learns on image pair, but transforms only second image123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Implements a spatial transformer layer as described in [1]_.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Based on [2]_ and edited by David Dao for Tensorflow.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Parameters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ----------123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    U : float123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        The output of a convolutional net should have the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        shape [num_batch, height, width, num_channels].123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    theta: float123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        The output of the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        localisation network should be [num_batch, 6].123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    out_size: tuple of two ints123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        The size of the output of the network (height, width)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    References123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ----------123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    .. [1]  Spatial Transformer Networks123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Submitted on 5 Jun 2015123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Notes123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    -----123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    To initialize the network to the identity transform init123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ``theta`` to :123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        identity = np.array([[1., 0., 0.],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                             [0., 1., 0.]])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        identity = identity.flatten()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        theta = tf.Variable(initial_value=identity)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _repeat(x, n_repeats):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('_repeat'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            rep = tf.transpose(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                tf.expand_dims(tf.ones(shape=tf.stack([n_repeats, ])), 1), [1, 0])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            rep = tf.cast(rep, 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x = tf.matmul(tf.reshape(x, (-1, 1)), rep)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return tf.reshape(x, [-1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _interpolate(im, x, y, out_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('_interpolate'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # constants123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            num_batch = tf.shape(im)[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            height = tf.shape(im)[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            width = tf.shape(im)[2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            channels = tf.shape(im)[3]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x = tf.cast(x, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y = tf.cast(y, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            height_f = tf.cast(height, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            width_f = tf.cast(width, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            out_height = out_size[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            out_width = out_size[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            zero = tf.zeros([], dtype='int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            max_y = tf.cast(tf.shape(im)[1] - 1, 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            max_x = tf.cast(tf.shape(im)[2] - 1, 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # scale indices from [-1, 1] to [0, width/height]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x = (x + 1.0) * (width_f) / 2.0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y = (y + 1.0) * (height_f) / 2.0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # do sampling123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x0 = tf.cast(tf.floor(x), 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x1 = x0 + 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y0 = tf.cast(tf.floor(y), 'int32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y1 = y0 + 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x0 = tf.clip_by_value(x0, zero, max_x)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x1 = tf.clip_by_value(x1, zero, max_x)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y0 = tf.clip_by_value(y0, zero, max_y)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y1 = tf.clip_by_value(y1, zero, max_y)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            dim2 = width123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            dim1 = width * height123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            base = _repeat(tf.range(num_batch) * dim1, out_height * out_width)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            base_y0 = base + y0 * dim2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            base_y1 = base + y1 * dim2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            idx_a = base_y0 + x0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            idx_b = base_y1 + x0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            idx_c = base_y0 + x1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            idx_d = base_y1 + x1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # use indices to lookup pixels in the flat image and restore123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # channels dim123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            im_flat = tf.reshape(im, tf.stack([-1, channels]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            im_flat = tf.cast(im_flat, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Ia = tf.gather(im_flat, idx_a)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Ib = tf.gather(im_flat, idx_b)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Ic = tf.gather(im_flat, idx_c)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            Id = tf.gather(im_flat, idx_d)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # and finally calculate interpolated values123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x0_f = tf.cast(x0, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x1_f = tf.cast(x1, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y0_f = tf.cast(y0, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y1_f = tf.cast(y1, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            wa = tf.expand_dims(((x1_f - x) * (y1_f - y)), 1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            wb = tf.expand_dims(((x1_f - x) * (y - y0_f)), 1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            wc = tf.expand_dims(((x - x0_f) * (y1_f - y)), 1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            wd = tf.expand_dims(((x - x0_f) * (y - y0_f)), 1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            output = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return output123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _meshgrid(height, width):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('_meshgrid'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # This should be equivalent to:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #                         np.linspace(-1, 1, height))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #  ones = np.ones(np.prod(x_t.shape))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                            tf.transpose(tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                            tf.ones(shape=tf.stack([1, width])))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x_t_flat = tf.reshape(x_t, (1, -1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y_t_flat = tf.reshape(y_t, (1, -1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            ones = tf.ones_like(x_t_flat)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return grid123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _transform(theta, input_dim, out_size, level):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.variable_scope('_transform'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if (level == 0):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                input_dim, _ = tf.split(input_dim, num_or_size_splits=2, axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            elif (level == 1):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                _, input_dim = tf.split(input_dim, num_or_size_splits=2, axis=3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            num_batch = tf.shape(input_dim)[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            height = tf.shape(input_dim)[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            width = tf.shape(input_dim)[2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            num_channels = tf.shape(input_dim)[3]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            theta = tf.reshape(theta, (-1, 2, 3))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            theta = tf.cast(theta, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # grid of (x_t, y_t, 1), eq (1) in ref [1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            height_f = tf.cast(height, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            width_f = tf.cast(width, 'float32')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            out_height = out_size[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            out_width = out_size[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = _meshgrid(out_height, out_width)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.expand_dims(grid, 0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.reshape(grid, [-1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.tile(grid, tf.stack([num_batch]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            grid = tf.reshape(grid, tf.stack([num_batch, 3, -1]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Transform A x (x_t, y_t, 1)^T -> (x_s, y_s)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            T_g = tf.matmul(theta, grid)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x_s = tf.slice(T_g, [0, 0, 0], [-1, 1, -1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y_s = tf.slice(T_g, [0, 1, 0], [-1, 1, -1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            x_s_flat = tf.reshape(x_s, [-1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y_s_flat = tf.reshape(y_s, [-1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            input_transformed = _interpolate(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                input_dim, x_s_flat, y_s_flat,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                out_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            output = tf.reshape(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return output123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.variable_scope(name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        output = _transform(theta, U, out_size, level_to_transform)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return output123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef batch_transformer(U, thetas, out_size, name='BatchSpatialTransformer'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Batch Spatial Transformer Layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Parameters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ----------123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    U : float123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tensor of inputs [num_batch,height,width,num_channels]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    thetas : float123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        a set of transformations for each input [num_batch,num_transforms,6]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    out_size : int123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        the size of the output [out_height,out_width]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :returns: float123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        Tensor of size [num_batch*num_transforms,out_height,out_width,num_channels]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.variable_scope(name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        num_batch, num_transforms = map(int, thetas.get_shape().as_list()[:2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        indices = [[i] * num_transforms for i in xrange(num_batch)]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        input_repeated = tf.gather(U, tf.reshape(indices, [-1]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return transformer(input_repeated, thetas, out_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef loss(logits, labels):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Calculates the cross-entropy loss.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Args:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      logits: Logits from inference().123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      labels: Labels from inputs(). 1-D tensor123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF              of shape [batch_size]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :returns:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      Loss tensor of type float.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Calculate the average cross entropy loss across the batch.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('cross_entropy_loss'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        labels = tf.cast(labels, tf.int64)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            labels=labels, logits=logits, name='cross_entropy_per_example')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.name_scope('total'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            tf.add_to_collection('losses', cross_entropy_mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.summary.scalar('cross_entropy_loss', cross_entropy_mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return cross_entropy_mean + 6 - 6123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef _add_loss_summaries(total_loss):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Add summaries for losses in CIFAR-10 model.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Generates moving average for all losses and associated summaries for123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    visualizing the performance of the network.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Args:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      total_loss: Total loss from loss().123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :returns:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      loss_averages_op: op for generating moving averages of losses.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Compute the moving average of all individual losses and the total loss.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    losses = tf.get_collection('losses')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    loss_averages_op = loss_averages.apply(losses + [total_loss])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Attach a scalar summary to all individual losses and the total loss; do the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # same for the averaged version of the losses.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    for l in losses + [total_loss]:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Name each loss as '(raw)' and name the moving average version of the loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # as the original loss name.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar(l.op.name + ' (raw)', l)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar(l.op.name, loss_averages.average(l))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return loss_averages_op123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef _activation_summary(x):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Helper to create summaries for activations.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Creates a summary that provides a histogram of activations.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Creates a summary that measures the sparsity of activations.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Args:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      x: Tensor123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Returns:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      nothing123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    author: The TensorFlow Authors123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # session. This helps the clarity of presentation on tensorboard.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.summary.histogram(tensor_name + '/activations', x)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.summary.scalar(tensor_name + '/sparsity',123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                      tf.nn.zero_fraction(x))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef _variable_on_cpu(name, shape, initializer):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Helper to create a Variable stored on CPU memory.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Args:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      name: name of the variable123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      shape: list of ints123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      initializer: initializer for Variable123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :returns:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      Variable Tensor123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.device('/cpu:0'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        dtype = tf.float16 if FLAGS.USE_FP16 else tf.float32123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return var123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef _variable_with_weight_decay(name, shape, stddev, wd):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Helper to create an initialized Variable with weight decay.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Note that the Variable is initialized with a truncated normal distribution.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    A weight decay is added only if one is specified.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Args:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      name: name of the variable123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      shape: list of ints123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      stddev: standard deviation of a truncated Gaussian123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      wd: add L2Loss weight decay multiplied by this float. If None, weight123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF          decay is not added for this Variable.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :returns:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      Variable Tensor123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    dtype = tf.float16 if FLAGS.USE_FP16 else tf.float32123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    var = _variable_on_cpu(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        name,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        shape,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    if wd is not None:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.name_scope(name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            tf.add_to_collection('losses', weight_decay)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            variable_summaries(weight_decay)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return var123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef dense_to_one_hot(labels, n_classes=2):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Convert class labels from scalars to one-hot vectors."""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    labels = np.array(labels)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    n_labels = labels.shape[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    index_offset = np.arange(n_labels) * n_classes123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    labels_one_hot = np.zeros((n_labels, n_classes), dtype=np.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    labels_one_hot.flat[index_offset + labels.ravel()] = 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return labels_one_hot123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef variable_summaries(var):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('summaries'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        mean = tf.reduce_mean(var)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('mean', mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.name_scope('stddev'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('stddev', stddev)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('max', tf.reduce_max(var))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('min', tf.reduce_min(var))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.histogram('histogram', var)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF