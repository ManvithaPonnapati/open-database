# Atomic Convolutions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 1 - Number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThese tests try to optimize the following parameters:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- learning rate123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below in the form: (learning rate, radial filters, epochs)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1a) 0.002, 5, 100 -- l2_loss = 0.6057 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.7946 (test), 0.6070 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1b) 0.001, 5, 150 -- l2_loss = 0.5911 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.8201 (test), 0.6199 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1c) 0.0005, 7, 250 -- l2_loss = 0.5205 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.8304 (test), 0.5927 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1d) 0.001, 7, 200 -- l2_loss = 0.5049 (still slowly converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.8516 (test), 0.6170 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1e) 0.0003, 7, 300 -- l2_loss = 0.5848 (still slowly converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.7728 (test), 0.5753 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFConclusion: Good L2 loss doesn't necessarily imply that the MUE will be good. It seems that the optimal amount of training is similar to in the paper. Small learning rate and large number of epochs is best.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 2 - Summing atomic energies instead of averaging123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn DeepChem's paper, they summed the atomic energies rather than averaging. I used averaging at first because my original architecture had different weights for protein, ligand, complex. If they share weights it makes sense to test summing. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below in the form (learning rate, radial filters, epochs, l2_loss)) - note this is the same as above123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2a) 0.002, 5, 100 -- l2_loss = 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2b) 0.0001, 5, 150 -- l2_loss = 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2c) 0.0005, 7, 250 -- l2_loss =123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2d) 0.001, 7, 200 -- l2_loss =123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2e) 0.0003, 7, 300 -- l2_loss =123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Things to test:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Run eval on the networks, assess performance, and then run more epochs.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Summing atomic energies instead of taking the mean123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Having different weights for ligand, protein, and complex123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Varying depth/width of FC layers123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Having different numbers of weights for ligand, protein, complex