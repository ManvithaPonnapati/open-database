# Atomic Convolutions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe benchmarks that DeepChem have set are (MUE [kcal/mmol]): 0.653 (test), 0.518 (train), using the PDBBind refined set with random split. The following parameters were used:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- 25 atom types, 5 radial filters, 12 neighbors, 12A radial cutoff123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- (128, 128, 64) pyramidal atomistic fully connected layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- 40% dropout, 24 batch size, 100 epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- summing the atomic energies123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFor more information about the benchmarks or network architecture, see DeepChem's paper [Atomic Convolutional Networks for Predicting Protein-Ligand Binding Affinity](https://arxiv.org/pdf/1703.10603.pdf).123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF*NOTE: It turns out that random guessing (always guessing the mean of the entire dataset) for the PDBBind refined test set gives a mean unsigned error of approximately **0.9630**. This a good benchmark to compare our results to.*123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 1 - Number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThese tests try to optimize the following parameters:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- learning rate123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below: 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | LR | Filters | Epochs | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | -- | ------- | ------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1a | 0.002 | 5 | 100 | 0.6057 | 0.7946 | 0.6070 | Still converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1b | 0.001 | 5 | 150 | 0.5911 | 0.7946 | 0.6199 | Still converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1c | 0.0005 | 7 | 250 | 0.5205 | 0.8304 | 0.5927 | Still converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1d | 0.001 | 7 | 200 | 0.5049 | 0.8516 | 0.6170 | Still slowly converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1e | 0.0003 | 7 | 300 | 0.5848 | **0.7728** | **0.5753** | Still slowly converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF**Conclusion**: All of the above experiments are still converging. Good L2 loss doesn't necessarily imply that the MUE will be good. It seems that the optimal amount of training is similar to in the paper. Small learning rate and large number of epochs is best.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 2 - Summing atomic energies instead of averaging123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn DeepChem's paper, they summed the atomic energies rather than averaging. I used averaging at first because my original architecture had different weights for protein, ligand, complex. If they share weights it makes sense to test summing. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below: 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | LR | Filters | Epochs | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | -- | ------- | ------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2a | 0.002 | 5 | 100 | 1.2839 | 0.9712 | 0.8508 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2b | 0.002 | 7 | 100 | 1.2815 | 0.9212 | 0.7882 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2c | 0.0005 | 7 | 200 | 1.3262 | 0.9549 | 0.7741 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2d | 0.0003 | 7 | 300 | 1.0761 | 0.9359 | 0.7207 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2e | 0.0003 | 5 | 300 | 1.1155 | 0.9554 | 0.7436 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2f | 0.0002 | 5 | 400 | 1.0577 | 0.9229 | 0.7336 | Still converging quickly |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2g | 0.0002 | 5 | 600 | 0.8859 | 0.9228 | 0.6702 | Converging slowly |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF**Conclusion**: Most still converging. Summing atomic energies doesn't really give the best results. I think I will try to stick with taking the mean, but with different weights for ligand, protein, and complex to account for scaling factors.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 3 - Using different weights for protein/ligand/complex123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe protein, ligand, and complex are fundamentally different and it makes sense to use different weights for them. This makes it more feasible to use mean energy instead of sum energy (which seems to be very inconsistent)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | LR | Filters | Epochs | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | -- | ------- | ------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 3a | 0.0002 | 5 | 356 | 0.4653 | **0.7368** | **0.5185** | Convergence rate: 0.04 per 50 epochs |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 3b | 0.0002 | 7 | 360 | 0.4510 | 0.8009 | 0.6093 | Convergence rate: 0.04 per 50 epochs |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 3c | 0.0002 | 5 | 371 | 0.4641 | 0.7646 | 0.5582 | Perhaps it's random variance, but more epochs doesn't seem to help much|123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 4 - Using different FC layer widths and depths123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn this experiment, we vary the number of FC layers and hidden units in each layer. Unless otherwise stated the other parameters are: (otherwise same as 3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- learning rate = 0.0002123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- radial filters = 5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFNote that the original architecture has a (128, 128, 64) atomistic FC pyramidal structure.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | Hidden Units | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | ------------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4a | (64, 64, 32) | 0.6089 | 0.7554 | 0.6333 | |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4b | (128, 64, 32) | 0.5168 | 0.7401 | 0.5690 | | 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4c | (256, 128, 64) | 0.3615 | 0.7614 | 0.5115 | High variance in MUE |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4d | (128, 64, 64, 32) | 0.4922 | 0.8469 | 0.6677 | Consistently bad, low variance MUE |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4e | (128, 128, 64, 64) | 0.4123 | 0.9353 | 0.7528 | Clear case of overfitting |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4f | (200, 100) | 0.5512 | **0.6928** | **0.5524** | 295 epochs |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4g | (160, 80) | 0.5889 | 0.6957 | 0.5782 | 310 epochs |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4h | (150, 100) | 0.5796 | 0.6988 | 0.5691 | 305 epochs |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4i | (256, 128) | 0.5308 | 0.6999 | 0.5318 | 268 epochs |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4j | (128, 64, 64) | 0.5758 | 0.7109 | 0.5722 | 274 epochs |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 5 - Adding an initial bias to the network 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn this experiment, we set a fixed bias to the output predicted energies. We add -3.9 to predicted energies to initially have predictions centered at the mean of the database. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThis tries to resolve the weird problem in which the majority of the output energies are slightly too high. The proportion of predictions which are too high is >75%.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | Hidden Units | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | ------------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 5a | (64, 64, 32) | | | | Job ID 617328 |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 5b | (128, 64, 32) | | | | Job ID 617329 |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 5c | (128, 64, 64) | | | | Job ID 617330 |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 5d | (128, 128, 64) | | | | Job ID 617331 |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 5e | (256, 128, 64) | | | | Job ID 617332 |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 5f | (128, 128, 64, 64) | | | | Job ID 617334 |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 5g | (256, 128) | | | | Job ID 617335 |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 5h | (180, 100) | | | | Job ID 617336 |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 5i | (128, 64) | | | | Job ID 617337 |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF###### IMPORTANT NOTE: Don't forget to add the same bias to eval to get the correct predictions!!!123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Things to test:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Resubmit 4f, 4g, 4h, 4i, 4j on titan.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Continue to optimize FC network 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try having the same weights for each atom type interaction (to reduce potential overfitting, perhaps). The difference between training/test error is very high. I should look into ways to reduce number of parameters. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Reintroduce BATCH NORMALIZATION into the network123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Add molecular FC layers at the end (that see the entire molecule)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Having different numbers of features for ligand, protein, complex123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Instead of having separate weights for ligand, protein, complex, just have separate final scaling factors when computing 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Trying the dataset with 25 atom types instead of 9 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#### Already done:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Write eval function to calculate MUE for test and train sets123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Optimize learning rate, radial filters, and number of epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try summing the atomic energies instead of taking the mean123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try different weights for protein, ligand, and complex to make taking the mean make more sense123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Write a script to determine the MUE of random guessing 