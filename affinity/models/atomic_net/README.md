# Atomic Convolutions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe benchmarks that DeepChem have set are (MUE [kcal/mmol]): 0.653 (test), 0.518 (train), using the PDBBind refined set with random split. The following parameters were used:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- 25 atom types, 5 radial filters, 12 neighbors, 12A radial cutoff123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- (128, 128, 64) pyramidal atomistic fully connected layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- 40% dropout, 24 batch size, 100 epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- summing the atomic energies123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFor more information about the benchmarks or network architecture, see DeepChem's paper [Atomic Convolutional Networks for Predicting Protein-Ligand Binding Affinity](https://arxiv.org/pdf/1703.10603.pdf).123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF*NOTE: It turns out that random guessing (always guessing the mean of the entire dataset) for the PDBBind refined test set gives a mean unsigned error of approximately **0.9630**. This a good benchmark to compare our results to.*123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 1 - Number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThese tests try to optimize the following parameters:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- learning rate123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | LR | Filters | Epochs | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | -- | ------- | ------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1a | 0.002 | 5 | 100 | 0.6057 | 0.7946 | 0.6070 | Still converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1b | 0.001 | 5 | 100 | 0.5911 | 0.7946 | 0.6199 | Still converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1c | 0.0005 | 7 | 250 | 0.5205 | 0.8304 | 0.5927 | Still converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1d | 0.001 | 7 | 200 | 0.5049 | 0.8516 | 0.6170 | Still slowly converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| **1e | 0.0003 | 7 | 300 | 0.5848 | 0.7728 | 0.5753 | Still slowly converging** |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1a) 0.002, 5, 100 -- l2_loss = 0.6057 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.7946 (test), 0.6070 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1b) 0.001, 5, 150 -- l2_loss = 0.5911 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.8201 (test), 0.6199 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1c) 0.0005, 7, 250 -- l2_loss = 0.5205 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.8304 (test), 0.5927 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1d) 0.001, 7, 200 -- l2_loss = 0.5049 (still slowly converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.8516 (test), 0.6170 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1e) 0.0003, 7, 300 -- l2_loss = 0.5848 (still slowly converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.7728 (test), 0.5753 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFConclusion: Good L2 loss doesn't necessarily imply that the MUE will be good. It seems that the optimal amount of training is similar to in the paper. Small learning rate and large number of epochs is best.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 2 - Summing atomic energies instead of averaging123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn DeepChem's paper, they summed the atomic energies rather than averaging. I used averaging at first because my original architecture had different weights for protein, ligand, complex. If they share weights it makes sense to test summing. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below in the form (learning rate, radial filters, epochs, l2_loss)) - note this is the same as above123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2a) 0.002, 5, 100 -- l2_loss = 1.2839 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9712 (test), 0.8508 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2b) 0.002, 7, 100 -- l2_loss = 1.2815 (approx same as 2a)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9212 (test), 0.7882 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2c) 0.0005, 5, 200 -- l2_loss = 1.3262 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9549 (test), 0.7741 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2d) 0.0003, 7, 300 -- l2_loss = 1.0761 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9359 (test), 0.7207 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2e) 0.0003, 5, 300 -- l2_loss = 1.1155 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9554 (test), 0.7436 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2f) 0.0002, 5, 400 -- l2_loss = 1.0577 (still converging pretty quickly)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9229 (test), 0.7336 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2g) 0.0002, 5, 600 -- l2_loss = 0.8859 (still converges but more slowly)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9228 (test), 0.6702 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFConclusion: Summing atomic energies doesn't really give the best results. I think I will try to stick with taking the mean, but with different weights for ligand, protein, and complex to account for scaling factors.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 3 - Using different weights for protein/ligand/complex123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe protein, ligand, and complex are fundamentally different and it makes sense to use different weights for them. This makes it more feasible to use mean energy instead of sum energy (which seems to be very inconsistent)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF3a) 0.0002, 5, 400 -- l2_loss =123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF3b) 0.0002, 7, 400 -- l2_loss =123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF3c) 0.0002, 5, 500 -- l2_loss =123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 4 - Using different FC layer widths and depths123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn this experiment, we vary the number of FC layers and hidden units in each layer. Unless otherwise stated the other parameters are: (everything else is the same as 3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- learning rate = 0.0002, num epochs = 400123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- radial filters = 5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFNote that the original architecture has a (128, 128, 64) atomistic FC pyramidal structure.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF4a) (64, 64, 32) -- l2_loss = 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF4b) (128, 64, 32) -- l2_loss =123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF4c) (256, 128, 64) -- l2_loss = 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF4d) (128, 64, 64, 32) -- l2_loss = 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF4e) (128, 128, 64, 64) -- l2_loss =123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF4f) (200, 100) -- l2_loss =123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Things to test:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try having the same weights for each atom type interaction (to reduce potential overfitting, perhaps)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Trying the dataset with 25 atom types instead of 9123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Add molecular FC layers at the end (that see the entire molecule)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Having different numbers of features for ligand, protein, complex123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#### Already done:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Optimize learning rate, radial filters, and number of epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try summing the atomic energies instead of taking the mean123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try different weights for protein, ligand, and complex to make taking the mean make more sense123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try to optimize FC network architecture at the end. 