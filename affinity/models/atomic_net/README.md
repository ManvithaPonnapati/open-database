# Atomic Convolutions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe benchmarks that DeepChem have set are (MUE [kcal/mmol]): 0.653 (test), 0.518 (train), using the PDBBind refined set with random split. The following parameters were used:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- 25 atom types, 5 radial filters, 12 neighbors, 12A radial cutoff123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- (128, 128, 64) pyramidal atomistic fully connected layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- 40% dropout, 24 batch size, 100 epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- summing the atomic energies123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFor more information about the benchmarks or network architecture, see DeepChem's paper [Atomic Convolutional Networks for Predicting Protein-Ligand Binding Affinity](https://arxiv.org/pdf/1703.10603.pdf).123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF*NOTE: It turns out that random guessing (always guessing the mean of the entire dataset) for the PDBBind refined test set gives a mean unsigned error of approximately **0.9630**. This a good benchmark to compare our results to.*123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 1 - Number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThese tests try to optimize the following parameters:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- learning rate123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below: 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | LR | Filters | Epochs | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | -- | ------- | ------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1a | 0.002 | 5 | 100 | 0.6057 | 0.7946 | 0.6070 | Still converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1b | 0.001 | 5 | 150 | 0.5911 | 0.7946 | 0.6199 | Still converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1c | 0.0005 | 7 | 250 | 0.5205 | 0.8304 | 0.5927 | Still converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1d | 0.001 | 7 | 200 | 0.5049 | 0.8516 | 0.6170 | Still slowly converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 1e | 0.0003 | 7 | 300 | 0.5848 | **0.7728** | **0.5753** | Still slowly converging |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF**Conclusion**: All of the above experiments are still converging. Good L2 loss doesn't necessarily imply that the MUE will be good. It seems that the optimal amount of training is similar to in the paper. Small learning rate and large number of epochs is best.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 2 - Summing atomic energies instead of averaging123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn DeepChem's paper, they summed the atomic energies rather than averaging. I used averaging at first because my original architecture had different weights for protein, ligand, complex. If they share weights it makes sense to test summing. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below: 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | LR | Filters | Epochs | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | -- | ------- | ------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2a | 0.002 | 5 | 100 | 1.2839 | 0.9712 | 0.8508 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2b | 0.002 | 7 | 100 | 1.2815 | 0.9212 | 0.7882 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2c | 0.0005 | 7 | 200 | 1.3262 | 0.9549 | 0.7741 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2d | 0.0003 | 7 | 300 | 1.0761 | 0.9359 | 0.7207 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2e | 0.0003 | 5 | 300 | 1.1155 | 0.9554 | 0.7436 |  |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2f | 0.0002 | 5 | 400 | 1.0577 | 0.9229 | 0.7336 | Still converging quickly |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 2g | 0.0002 | 5 | 600 | 0.8859 | 0.9228 | 0.6702 | Converging slowly |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF**Conclusion**: Most still converging. Summing atomic energies doesn't really give the best results. I think I will try to stick with taking the mean, but with different weights for ligand, protein, and complex to account for scaling factors.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 3 - Using different weights for protein/ligand/complex123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe protein, ligand, and complex are fundamentally different and it makes sense to use different weights for them. This makes it more feasible to use mean energy instead of sum energy (which seems to be very inconsistent)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | LR | Filters | Epochs | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | -- | ------- | ------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 3a | 0.0002 | 5 | 356 | 0.4653 | **0.7368** | **0.5185** | Convergence rate: 0.04 per 50 epochs |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 3b | 0.0002 | 7 | 360 | 0.4510 | 0.8009 | 0.6093 | Convergence rate: 0.04 per 50 epochs |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 3c | 0.0002 | 5 | 371 | 0.4641 | 0.7646 | 0.5582 | Perhaps it's random variance, but more epochs doesn't seem to help much|123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 4 - Using different FC layer widths and depths123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn this experiment, we vary the number of FC layers and hidden units in each layer. Unless otherwise stated the other parameters are: (otherwise same as 3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- learning rate = 0.0002123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- radial filters = 5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFNote that the original architecture has a (128, 128, 64) atomistic FC pyramidal structure.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| Test | Hidden Units | L2 Loss | MUE (test) | MUE (train) | Notes |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| ---- | ------------ | ------- | ---------- | ----------- | ----- |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4a | (64, 64, 32) | 0.6089 | 0.7554 | 0.6333 | |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4b | (128, 64, 32) | 0.5168 | 0.7401 | 0.5690 | | 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4c | (256, 128, 64) | 0.3615 | 0.7614 | 0.5115 | Tends to do well most of the time on test set, but sometimes has huge MUE |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4d | (128, 64, 64, 32) | 0.4922 | 0.8469 | 0.6677 | Error is pretty consistently high - low variance in MUE |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4e | (128, 128, 64, 64) | 0.4123 | 0.9353 | 0.7528 | Clear case of overfitting |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF| 4f | (200, 100) | | | | |123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Things to test:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Take a closer look at experiment 4e. The results of the train evaluation doesn't seem to match up with the training log. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Continue to optimize FC network 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try having the same weights for each atom type interaction (to reduce potential overfitting, perhaps). The difference between training/test error is very high. I should look into ways to reduce number of parameters. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Reintroduce BATCH NORMALIZATION into the network123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Add molecular FC layers at the end (that see the entire molecule)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Having different numbers of features for ligand, protein, complex123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Instead of having separate weights for ligand, protein, complex, just have separate final scaling factors when computing 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Trying the dataset with 25 atom types instead of 9 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#### Already done:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Write eval function to calculate MUE for test and train sets123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Optimize learning rate, radial filters, and number of epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try summing the atomic energies instead of taking the mean123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try different weights for protein, ligand, and complex to make taking the mean make more sense123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Write a script to determine the MUE of random guessing 