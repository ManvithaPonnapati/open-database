# Atomic Convolutions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe benchmarks that DeepChem have set are (MUE [kcal/mmol]): 0.653 (test), 0.518 (train), using the PDBBind refined set with random split. The following parameters were used:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- 25 atom types, 5 radial filters, 12 neighbors, 12A radial cutoff123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- (128, 128, 64) pyramidal atomistic fully connected layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- 40% dropout, 24 batch size, 100 epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- summing the atomic energies123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 1 - Number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThese tests try to optimize the following parameters:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- number of radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- learning rate123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below in the form: (learning rate, radial filters, epochs)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1a) 0.002, 5, 100 -- l2_loss = 0.6057 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.7946 (test), 0.6070 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1b) 0.001, 5, 150 -- l2_loss = 0.5911 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.8201 (test), 0.6199 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1c) 0.0005, 7, 250 -- l2_loss = 0.5205 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.8304 (test), 0.5927 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1d) 0.001, 7, 200 -- l2_loss = 0.5049 (still slowly converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.8516 (test), 0.6170 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1e) 0.0003, 7, 300 -- l2_loss = 0.5848 (still slowly converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.7728 (test), 0.5753 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFConclusion: Good L2 loss doesn't necessarily imply that the MUE will be good. It seems that the optimal amount of training is similar to in the paper. Small learning rate and large number of epochs is best.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 2 - Summing atomic energies instead of averaging123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn DeepChem's paper, they summed the atomic energies rather than averaging. I used averaging at first because my original architecture had different weights for protein, ligand, complex. If they share weights it makes sense to test summing. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFParameters/results are listed below in the form (learning rate, radial filters, epochs, l2_loss)) - note this is the same as above123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2a) 0.002, 5, 100 -- l2_loss = 1.2839 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9712 (test), 0.8508 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2b) 0.002, 7, 100 -- l2_loss = 1.2815 (approx same as 2a)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9212 (test), 0.7882 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2c) 0.0005, 5, 200 -- l2_loss = 1.3262 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9549 (test), 0.7741 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2d) 0.0003, 7, 300 -- l2_loss = 1.0761 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9359 (test), 0.7207 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2e) 0.0003, 5, 300 -- l2_loss = 1.1155 (still converging)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9554 (test), 0.7436 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2f) 0.0002, 5, 400 -- l2_loss = 1.0577 (still converging pretty quickly)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- MUE: 0.9229 (test), 0.7336 (train)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFConclusion: Summing atomic energies doesn't really give the best results. I think I will try to stick with taking the mean, but with different weights for ligand, protein, and complex to account for scaling factors.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 3 - Using different weights for protein/ligand/complex123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe protein, ligand, and complex are fundamentally different and it makes sense to use different weights for them. This makes it more feasible to use mean energy instead of sum energy (which seems to be very inconsistent)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Things to test:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Try training the sum method (experiment 2) on more epochs to see if performance can improve123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Trying the dataset with 25 atom types instead of 9123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Having different weights for ligand, protein, and complex123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Varying depth/width of FC layers123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF- Having different numbers of features for ligand, protein, complex