import time,os,sys123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#sys.path.append("../../")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#from af_input import av4_input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#from af_networks import av4_networks123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#import av4_input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#import av4_networks123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#import .. affinity123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFsys.path.append(os.path.join(os.path.dirname(__file__), "../../.."))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport affinity as af123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom config import FLAGS123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef train():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    "train a network"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # it's better if all of the computations use a single session123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    sess = tf.Session()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create a filename queue first123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    filename_q, examples_in_database = af.input.index_database_into_q(FLAGS.train_data_path, shuffle=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create an epoch counter123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.variable_scope("epoch_counter"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        batch_counter = tf.Variable(0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        batch_counter_increment = tf.assign(batch_counter,tf.Variable(0).count_up_to(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            np.round((examples_in_database*FLAGS.train_epochs)/FLAGS.batch_size)))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        epoch_counter = tf.div(batch_counter*FLAGS.batch_size,examples_in_database)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create a custom shuffle queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    _,current_epoch,label_batch,image_batch = af.input.image_and_label_q(batch_size=FLAGS.batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                          pixel_size=FLAGS.pixel_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                          side_pixels=FLAGS.side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                          num_threads=FLAGS.num_threads,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                          filename_q=filename_q,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                          epoch_counter=epoch_counter,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                          lig_frame='OVERSAMPLING')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    keep_prob = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.variable_scope("network"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        max_net = af.networks.MaxNet()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        predicted_labels = max_net.compute_output(image_batch,keep_prob,FLAGS.batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label_batch,logits=predicted_labels)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    cross_entropy_mean = tf.reduce_mean(cross_entropy)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.summary.scalar('cross entropy mean', cross_entropy_mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # randomly shuffle along the batch dimension and calculate an error123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    shuffled_labels = tf.random_shuffle(label_batch)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    shuffled_cross_entropy_mean = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predicted_labels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                                labels=shuffled_labels))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.summary.scalar('shuffled cross entropy mean', shuffled_cross_entropy_mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Adam optimizer is a very heart of the network123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.variable_scope("Adam_optimizer"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        train_step_run = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # merge all summaries and create a file writer object123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    merged_summaries = tf.summary.merge_all()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    train_writer = tf.summary.FileWriter((FLAGS.summaries_dir + '/' + FLAGS.run_name + "_logs"), sess.graph)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create saver to save and load the network state123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    saver = tf.train.Saver(var_list=(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope="Adam_optimizer") +123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                     tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope="network") +123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                     tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope="epoch_counter")))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    if FLAGS.train_saved_sess is None:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        sess.run(tf.global_variables_initializer())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "Restoring variables from sleep. This may take a while..."123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # sess.run(tf.global_variables_initializer())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        saver.restore(sess,FLAGS.train_saved_sess)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "all variables restored. Start training"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    coord = tf.train.Coordinator()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    threads = tf.train.start_queue_runners(sess=sess, coord=coord)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    while True:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        start = time.time()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        batch_num = sess.run(batch_counter_increment)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        epo,c_entropy_mean,_ = sess.run([current_epoch,cross_entropy_mean,train_step_run], feed_dict={keep_prob: 0.5})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "epoch:",epo[0],"global step:", batch_num, "\tcross entropy mean:", c_entropy_mean,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "\texamples per second:", "%.2f" % (FLAGS.batch_size / (time.time() - start))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if (batch_num % 10 == 9):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # once in a while save the network state and write variable summaries to disk123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            c_entropy_mean,sc_entropy_mean,summaries = sess.run(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                [cross_entropy_mean, shuffled_cross_entropy_mean, merged_summaries], feed_dict={keep_prob: 1})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "cross entropy mean:",c_entropy_mean, "shuffled cross entropy mean:", sc_entropy_mean123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            train_writer.add_summary(summaries, batch_num)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            saver.save(sess, FLAGS.summaries_dir + '/' + FLAGS.run_name + "_netstate/saved_state", global_step=batch_num)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    assert not np.isnan(cross_entropy_mean), 'Model diverged with loss = NaN'123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef main(_):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Create directoris two directories: one for the logs, one for the network state. Start the script.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    summaries_dir = os.path.join(FLAGS.summaries_dir)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # FLAGS.run_index defines when123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    FLAGS.run_index = 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    FLAGS.run_name = FLAGS.run_name + "_main"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    while tf.gfile.Exists(summaries_dir + "/" + str(FLAGS.run_index) +'_netstate') \123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            or tf.gfile.Exists(summaries_dir + "/" + str(FLAGS.run_index)+'_logs'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        FLAGS.run_index += 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    FLAGS.run_name = str(FLAGS.run_index) + "_" + FLAGS.run_name123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.gfile.MakeDirs(summaries_dir + "/" + FLAGS.run_name +'_netstate')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.gfile.MakeDirs(summaries_dir + "/" + FLAGS.run_name +'_logs')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    train()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFif __name__ == '__main__':123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.app.run()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF