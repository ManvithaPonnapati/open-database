# Concat Net123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 4123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFI want to find out what gives this network such a good performance.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFtry:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1) Hiding some of the interactions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2) Turning off the ReLUs completely and substituting them with CReLUs (better, Sigmoid act,and something else)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFI have a weak suspicion that the networks might have an overfit even with a single layer. I am repeating experiments and adding a dropout123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV2 -> V13 (1 11x11x11)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV3 -> V14 (0.5 11x11x11)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV4 -> V15 (0.3 11x11x11)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV5 -> V16 (0.5 21x21x21)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV6 -> V17 (0.3 21x21x21)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV12 -> V18 (pairlist = 0, 0.5 11x11x11)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV13:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF@20K123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFno drop ave: 0.247734 std: 0.00767842123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdropout ave: 0.289957 std: 0.0582667123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV14:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF@55K123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFno drop ave: 0.0941794 std: 0.0282959123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdropout ave: 0.128597 std: 0.0443598123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV15:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF@100K123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFno drop ave: 0.0600498 std: 0.00621744123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdropout ave: 0.143919 std: 0.0589839123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV16:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF@17K123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFno drop ave: 0.0541315 std: 0.0279311123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdropout ave: 0.0997609 std: 0.0530068123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV17:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF@40K123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFno drop ave: 0.00946416 std: 0.00649887123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdropout ave: 0.0427706 std: 0.0973454123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV18:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF@100K123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFno drop ave: 0.474544 std: 0.0335041123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdropout ave: 0.493018 std: 0.0462594123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFI found a bug in tensorflow. It appears that when running on GPU tf.gather is unable to raise an exception. More here: https://github.com/tensorflow/tensorflow/issues/3638 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFI had two bugs in my own functions 1) Forgot to shift protein to 0 after shifting it in the process of cropping the binding site 2) concat nonlinear convolution had incorrect gather index all the time123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV1 Will give an error on the CPU now. Impossible123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV3 -> V7 (0.5 11x11x11)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV3 -> V8 (0.5 11x11x11)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV4 -> V9 (0.3 11x11x11)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV5 -> V10 (0.5 21x21x21)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV6 -> V11 (pairlist_distance = 0 instead of 2.5 0.5 11x11x11) 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV12 pixel size 0.5 pairlist_distance: 0.001123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFSince I forgot to set up the variable summaries, including two checkpoints (~12, ~15 hours)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV7 ave 0.04 std 0.038		| ave 0.032 std 0.029 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV8 ave 0.04 std 0.036		| ave 0.019 std 0.022123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV9 ave 0.034 std 0.038		| ave 0.017 std 0.02123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV10 (typo)                      | ave 0.026 std 0.024123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV11 ave 0.002 std 0.006		| ave 0.006 std 0.019123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV12 ave 0.46 std 0.042		| ave 0.45  std 0.049123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF### Experiment 1 Pixel Size/Cutoff123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIt is an optimization of a single-layer network consisting of tf.nn.concat_nonlinear_conv3d() 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFconvolutional layer + FC + FC.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFTrying different pixel sizes and cuttoffs. The only parameters changing are:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1) Pixel size in the first (single) convolutional layer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF2) Cutoff for the pairlist distance123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV1: pixel size: 0.5 pairlist distance: 5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV2: pixel size: 1 pairlist distance: 5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV3: pixel size: 0.5 pairlist distance: 2.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV4: pixel size: 0.3 pairlist distance 1.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV5: pixel size: 0.5 pairlist distance 5 ( I had to substitute convolution size 11x11x11 to 21x21x21 in layer 1) 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV6: pixel size: 0.3 pairlist distance 3 ( I had to substitute convolution size 11x11x11 to 21x21x21 in layer 1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFCross entropy: 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV1: 0.52 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV2: 0.45123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV3: 0.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV4: 0.43123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV5: 0.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV6: 0.44123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFrom python notebook since fluctuations seem to be very large:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV1: mean: 0.54 std: 0.05123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV2: mean: 0.47 std: 0.052123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV3: mean: 0.48 std: 0.05123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV4: mean: 0.47 std: 0.05123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV5: mean: 0.51 std: 0.053123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFV6: mean: 0.46 std: 0.07 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF