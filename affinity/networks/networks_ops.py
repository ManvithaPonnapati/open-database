import time, sys123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport affinity.geom123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport affinity.input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport affinity.nn123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# telling tensorflow how we want to randomly initialize weights123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef weight_variable(shape):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = tf.truncated_normal(shape, stddev=0.005)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return tf.Variable(initial)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef bias_variable(shape):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = tf.constant(0.01, shape=shape)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return tf.Variable(initial)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef bias_variable_with_initializer(shape, initializer=0.01):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    initial = tf.constant(0.01, shape=shape)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return tf.Variable(initial)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef variable_summaries(var, name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """attaches a lot of summaries to a tensor."""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('summaries'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        mean = tf.reduce_mean(var)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('mean/' + name, mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('stddev'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('stddev/' + name, stddev)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('max/' + name, tf.reduce_max(var))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('min/' + name, tf.reduce_min(var))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.histogram(name, var)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef conv_layer(layer_name, input_tensor, filter_size, strides=[1, 1, 1, 1, 1], padding='SAME'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """makes a simple convolutional layer"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    input_depth = filter_size[3]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    output_depth = filter_size[4]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope(layer_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.name_scope('weights'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            W_conv = weight_variable(filter_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            variable_summaries(W_conv, layer_name + '/weights')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.name_scope('biases'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            b_conv = bias_variable([output_depth])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            variable_summaries(b_conv, layer_name + '/biases')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            h_conv = tf.nn.conv3d(input_tensor, W_conv, strides=strides, padding=padding) + b_conv123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            tf.summary.histogram(layer_name + '/pooling_output', h_conv)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print layer_name, "output dimensions:", h_conv.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return h_conv123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef relu_layer(layer_name, input_tensor, act=tf.nn.relu):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """makes a simple relu layer"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope(layer_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu = act(input_tensor, name='activation')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.histogram(layer_name + '/relu_output', h_relu)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar(layer_name + '/sparsity', tf.nn.zero_fraction(h_relu))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print layer_name, "output dimensions:", h_relu.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return h_relu123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef pool_layer(layer_name, input_tensor, ksize, strides=[1, 1, 1, 1, 1], padding='SAME'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """makes a simple max pooling layer"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope(layer_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool = tf.nn.max_pool3d(input_tensor, ksize=ksize, strides=strides, padding=padding)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.histogram(layer_name + '/max_pooling_output', h_pool)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print layer_name, "output dimensions:", h_pool.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return h_pool123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef avg_pool_layer(layer_name, input_tensor, ksize, strides=[1, 1, 1, 1, 1], padding='SAME'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """makes a average pooling layer"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope(layer_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool = tf.nn.avg_pool3d(input_tensor, ksize=ksize, strides=strides, padding=padding)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.histogram(layer_name + '/average_pooling_output', h_pool)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print layer_name, "output dimensions:", h_pool.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return h_pool123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef fc_layer(layer_name, input_tensor, output_dim):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """makes a simple fully connected layer"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    input_dim = int((input_tensor.get_shape())[1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope(layer_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        weights = weight_variable([input_dim, output_dim])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        variable_summaries(weights, layer_name + '/weights')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('biases'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        biases = bias_variable([output_dim])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        variable_summaries(biases, layer_name + '/biases')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('Wx_plus_b'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc = tf.matmul(input_tensor, weights) + biases123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.histogram(layer_name + '/fc_output', h_fc)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print layer_name, "output dimensions:", h_fc.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return h_fc123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef old_max_net(x_image_batch, keep_prob, batch_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    "makes a simple network that can receive 20x20x20 input images. And output 2 classes"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('input'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        pass123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("input_reshape"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "image batch dimensions", x_image_batch.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # formally adding one depth dimension to the input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        x_image_batch = tf.reshape(x_image_batch, [batch_size, 20, 20, 20, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "input to the first layer dimensions", x_image_batch.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv1 = conv_layer(layer_name='conv1_5x5x5', input_tensor=x_image_batch, filter_size=[5, 5, 5, 1, 20])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu1 = relu_layer(layer_name='relu1', input_tensor=h_conv1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool1 = pool_layer(layer_name='pool1_2x2x2', input_tensor=h_relu1, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv2 = conv_layer(layer_name="conv2_3x3x3", input_tensor=h_pool1, filter_size=[3, 3, 3, 20, 30])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu2 = relu_layer(layer_name="relu2", input_tensor=h_conv2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool2 = pool_layer(layer_name="pool2_2x2x2", input_tensor=h_relu2, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv3 = conv_layer(layer_name="conv3_2x2x2", input_tensor=h_pool2, filter_size=[2, 2, 2, 30, 40])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu3 = relu_layer(layer_name="relu3", input_tensor=h_conv3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool3 = pool_layer(layer_name="pool3_2x2x2", input_tensor=h_relu3, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv4 = conv_layer(layer_name="conv4_2x2x2", input_tensor=h_pool3, filter_size=[2, 2, 2, 40, 50])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu4 = relu_layer(layer_name="relu4", input_tensor=h_conv4)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool4 = pool_layer(layer_name="pool4_2x2x2", input_tensor=h_relu4, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv5 = conv_layer(layer_name="conv5_2x2x2", input_tensor=h_pool4, filter_size=[2, 2, 2, 50, 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu5 = relu_layer(layer_name="relu5", input_tensor=h_conv5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool5 = pool_layer(layer_name="pool5_2x2x2", input_tensor=h_relu5, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("flatten_layer"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool2_flat = tf.reshape(h_pool5, [-1, 5 * 5 * 5 * 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1 = fc_layer(layer_name="fc1", input_tensor=h_pool2_flat, output_dim=1024)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1_relu = relu_layer(layer_name="fc1_relu", input_tensor=h_fc1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("dropout"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('dropout_keep_probability', keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1_drop = tf.nn.dropout(h_fc1_relu, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc2 = fc_layer(layer_name="fc2", input_tensor=h_fc1_drop, output_dim=256)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc2_relu = relu_layer(layer_name="fc2_relu", input_tensor=h_fc2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    y_conv = fc_layer(layer_name="out_neuron", input_tensor=h_fc2_relu, output_dim=2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return y_conv123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef ag_net(x_image_batch, keep_prob, batch_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    "makes a simple network that can receive 20x20x20 input images. And output 2 classes"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('input'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        pass123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("input_reshape"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "image batch dimensions", x_image_batch.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # formally adding one depth dimension to the input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        x_image_batch = tf.reshape(x_image_batch, [batch_size, 20, 20, 20, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "input to the first layer dimensions", x_image_batch.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv1 = conv_layer(layer_name='conv1_5x5x5', input_tensor=x_image_batch, filter_size=[5, 5, 5, 1, 20])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu1 = relu_layer(layer_name='relu1', input_tensor=h_conv1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool1 = pool_layer(layer_name='pool1_2x2x2', input_tensor=h_relu1, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv2 = conv_layer(layer_name="conv2_3x3x3", input_tensor=h_pool1, filter_size=[3, 3, 3, 20, 30])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu2 = relu_layer(layer_name="relu2", input_tensor=h_conv2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool2 = pool_layer(layer_name="pool2_2x2x2", input_tensor=h_relu2, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv3 = conv_layer(layer_name="conv3_2x2x2", input_tensor=h_pool2, filter_size=[2, 2, 2, 30, 40])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu3 = relu_layer(layer_name="relu3", input_tensor=h_conv3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool3 = pool_layer(layer_name="pool3_2x2x2", input_tensor=h_relu3, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv4 = conv_layer(layer_name="conv4_2x2x2", input_tensor=h_pool3, filter_size=[2, 2, 2, 40, 50])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu4 = relu_layer(layer_name="relu4", input_tensor=h_conv4)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool4 = pool_layer(layer_name="pool4_2x2x2", input_tensor=h_relu4, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv5 = conv_layer(layer_name="conv5_2x2x2", input_tensor=h_pool4, filter_size=[2, 2, 2, 50, 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu5 = relu_layer(layer_name="relu5", input_tensor=h_conv5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool5 = pool_layer(layer_name="pool5_2x2x2", input_tensor=h_relu5, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("flatten_layer"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool2_flat = tf.reshape(h_pool5, [-1, 5 * 5 * 5 * 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1 = fc_layer(layer_name="fc1", input_tensor=h_pool2_flat, output_dim=1024)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1_relu = relu_layer(layer_name="fc1_relu", input_tensor=h_fc1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("dropout"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('dropout_keep_probability', keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1_drop = tf.nn.dropout(h_fc1_relu, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc2 = fc_layer(layer_name="fc2", input_tensor=h_fc1_drop, output_dim=256)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc2_relu = relu_layer(layer_name="fc2_relu", input_tensor=h_fc2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    y_conv = fc_layer(layer_name="out_neuron", input_tensor=h_fc2_relu, output_dim=2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return y_conv  # tf.reshape(y_conv,[batch_size,1,3])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef ag_net_2(x_image_batch, keep_prob, batch_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    "makes a simple network that can receive 20x20x20 input images. And output 2 classes"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('input'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        pass123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("input_reshape"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "image batch dimensions", x_image_batch.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # formally adding one depth dimension to the input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # x_image_batch = tf.reshape(x_image_batch, [batch_size, 20, 20, 20, 2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "input to the first layer dimensions", x_image_batch.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv1 = conv_layer(layer_name='conv1_5x5x5', input_tensor=x_image_batch, filter_size=[5, 5, 5, 2, 20])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu1 = relu_layer(layer_name='relu1', input_tensor=h_conv1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool1 = pool_layer(layer_name='pool1_2x2x2', input_tensor=h_relu1, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv2 = conv_layer(layer_name="conv2_3x3x3", input_tensor=h_pool1, filter_size=[3, 3, 3, 20, 30])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu2 = relu_layer(layer_name="relu2", input_tensor=h_conv2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool2 = pool_layer(layer_name="pool2_2x2x2", input_tensor=h_relu2, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv3 = conv_layer(layer_name="conv3_2x2x2", input_tensor=h_pool2, filter_size=[2, 2, 2, 30, 40])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu3 = relu_layer(layer_name="relu3", input_tensor=h_conv3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool3 = pool_layer(layer_name="pool3_2x2x2", input_tensor=h_relu3, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv4 = conv_layer(layer_name="conv4_2x2x2", input_tensor=h_pool3, filter_size=[2, 2, 2, 40, 50])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu4 = relu_layer(layer_name="relu4", input_tensor=h_conv4)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool4 = pool_layer(layer_name="pool4_2x2x2", input_tensor=h_relu4, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv5 = conv_layer(layer_name="conv5_2x2x2", input_tensor=h_pool4, filter_size=[2, 2, 2, 50, 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu5 = relu_layer(layer_name="relu5", input_tensor=h_conv5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool5 = pool_layer(layer_name="pool5_2x2x2", input_tensor=h_relu5, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("flatten_layer"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool2_flat = tf.reshape(h_pool5, [-1, 5 * 5 * 5 * 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1 = fc_layer(layer_name="fc1", input_tensor=h_pool2_flat, output_dim=1024)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1_relu = relu_layer(layer_name="fc1_relu", input_tensor=h_fc1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("dropout"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('dropout_keep_probability', keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1_drop = tf.nn.dropout(h_fc1_relu, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc2 = fc_layer(layer_name="fc2", input_tensor=h_fc1_drop, output_dim=256)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc2_relu = relu_layer(layer_name="fc2_relu", input_tensor=h_fc2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    y_conv = fc_layer(layer_name="out_neuron", input_tensor=h_fc2_relu, output_dim=2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return tf.reshape(y_conv, [batch_size, 1, 2])  # y_conv123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef ag_net_3(x_image_batch, keep_prob, batch_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    "makes a simple network that can receive 20x20x20 input images. And output 2 classes"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope('input'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        pass123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("input_reshape"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "image batch dimensions", x_image_batch.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # formally adding one depth dimension to the input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # x_image_batch = tf.reshape(x_image_batch, [batch_size, 20, 20, 20, 2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "input to the first layer dimensions", x_image_batch.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv1 = conv_layer(layer_name='conv1_5x5x5', input_tensor=x_image_batch, filter_size=[7, 7, 7, 2, 20])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu1 = relu_layer(layer_name='relu1', input_tensor=h_conv1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool1 = pool_layer(layer_name='pool1_2x2x2', input_tensor=h_relu1, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv2 = conv_layer(layer_name="conv2_3x3x3", input_tensor=h_pool1, filter_size=[5, 5, 5, 20, 30])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu2 = relu_layer(layer_name="relu2", input_tensor=h_conv2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool2 = pool_layer(layer_name="pool2_2x2x2", input_tensor=h_relu2, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv3 = conv_layer(layer_name="conv3_2x2x2", input_tensor=h_pool2, filter_size=[3, 3, 3, 30, 40])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu3 = relu_layer(layer_name="relu3", input_tensor=h_conv3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool3 = pool_layer(layer_name="pool3_2x2x2", input_tensor=h_relu3, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv4 = conv_layer(layer_name="conv4_2x2x2", input_tensor=h_pool3, filter_size=[3, 3, 3, 40, 50])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu4 = relu_layer(layer_name="relu4", input_tensor=h_conv4)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool4 = pool_layer(layer_name="pool4_2x2x2", input_tensor=h_relu4, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_conv5 = conv_layer(layer_name="conv5_2x2x2", input_tensor=h_pool4, filter_size=[2, 2, 2, 50, 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_relu5 = relu_layer(layer_name="relu5", input_tensor=h_conv5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_pool5 = pool_layer(layer_name="pool5_2x2x2", input_tensor=h_relu5, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 1, 1, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("flatten_layer"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool2_flat = tf.reshape(h_pool5, [-1, 10 * 10 * 10 * 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1 = fc_layer(layer_name="fc1", input_tensor=h_pool2_flat, output_dim=1024)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc1_relu = relu_layer(layer_name="fc1_relu", input_tensor=h_fc1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("dropout"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tf.summary.scalar('dropout_keep_probability', keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1_drop = tf.nn.dropout(h_fc1_relu, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc2 = fc_layer(layer_name="fc2", input_tensor=h_fc1_drop, output_dim=256)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    h_fc2_relu = relu_layer(layer_name="fc2_relu", input_tensor=h_fc2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    y_conv = fc_layer(layer_name="out_neuron", input_tensor=h_fc2_relu, output_dim=6)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return tf.reshape(y_conv, [batch_size, 3, 2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass MaxNet:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # todo: write tf file to monitor weights and biases acts123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def __init__(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w1 = weight_variable([5, 5, 5, 1, 20])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w2 = weight_variable([3, 3, 3, 20, 30])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w3 = weight_variable([2, 2, 2, 30, 40])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w4 = weight_variable([2, 2, 2, 40, 50])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w5 = weight_variable([2, 2, 2, 50, 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b1 = bias_variable([20])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b2 = bias_variable([30])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b3 = bias_variable([40])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b4 = bias_variable([50])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b5 = bias_variable([60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1w = weight_variable([7500, 1024])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1b = bias_variable([1024])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc2w = weight_variable([1024, 256])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc2b = bias_variable([256])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc3w = weight_variable([256, 2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc3b = bias_variable([2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def compute_output(self, image_batch, keep_prob, batch_size, side_pixels=20):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        x_image_batch = tf.reshape(image_batch, [batch_size, side_pixels, side_pixels, side_pixels, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv1 = tf.nn.conv3d(x_image_batch, self.w1, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu1 = tf.nn.relu(h_conv1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool1 = tf.nn.max_pool3d(h_relu1, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv2 = tf.nn.conv3d(h_pool1, self.w2, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu2 = tf.nn.relu(h_conv2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool2 = tf.nn.max_pool3d(h_relu2, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv3 = tf.nn.conv3d(h_pool2, self.w3, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu3 = tf.nn.relu(h_conv3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool3 = tf.nn.max_pool3d(h_relu3, ksize=[1, 1, 1, 1, 1], strides=[1, 1, 1, 1, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv4 = tf.nn.conv3d(h_pool3, self.w4, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b4123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu4 = tf.nn.relu(h_conv4)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool4 = tf.nn.max_pool3d(h_relu4, ksize=[1, 1, 1, 1, 1], strides=[1, 1, 1, 1, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv5 = tf.nn.conv3d(h_pool4, self.w5, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu5 = tf.nn.relu(h_conv5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool5 = tf.nn.max_pool3d(h_relu5, ksize=[1, 1, 1, 1, 1], strides=[1, 1, 1, 1, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool2_flat = tf.reshape(h_pool5, [-1, 5 * 5 * 5 * 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, self.fc1w) + self.fc1b)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, self.fc2w) + self.fc2b)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        y_conv = tf.matmul(h_fc2, self.fc3w) + self.fc3b123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return y_conv123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass NilaiNet:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # todo: write tf file to monitor weights and biases acts123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def __init__(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w1 = weight_variable([5, 5, 5, 1, 20])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w2 = weight_variable([3, 3, 3, 20, 30])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w3 = weight_variable([2, 2, 2, 30, 40])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w4 = weight_variable([2, 2, 2, 40, 50])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.w5 = weight_variable([2, 2, 2, 50, 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b1 = bias_variable([20])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b2 = bias_variable([30])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b3 = bias_variable([40])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b4 = bias_variable([50])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b5 = bias_variable([60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1w = weight_variable([60000, 1024])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1b = bias_variable([1024])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc2w = weight_variable([1024, 256])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc2b = bias_variable([256])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc3w = weight_variable([256, 2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc3b = bias_variable([2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def compute_output(self, image_batch, keep_prob, batch_size, side_pixels=40):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        x_image_batch = tf.reshape(image_batch, [batch_size, side_pixels, side_pixels, side_pixels, 1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv1 = tf.nn.conv3d(x_image_batch, self.w1, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu1 = tf.nn.relu(h_conv1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool1 = tf.nn.max_pool3d(h_relu1, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv2 = tf.nn.conv3d(h_pool1, self.w2, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu2 = tf.nn.relu(h_conv2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool2 = tf.nn.max_pool3d(h_relu2, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv3 = tf.nn.conv3d(h_pool2, self.w3, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu3 = tf.nn.relu(h_conv3)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool3 = tf.nn.max_pool3d(h_relu3, ksize=[1, 1, 1, 1, 1], strides=[1, 1, 1, 1, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv4 = tf.nn.conv3d(h_pool3, self.w4, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b4123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu4 = tf.nn.relu(h_conv4)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool4 = tf.nn.max_pool3d(h_relu4, ksize=[1, 1, 1, 1, 1], strides=[1, 1, 1, 1, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_conv5 = tf.nn.conv3d(h_pool4, self.w5, strides=[1, 1, 1, 1, 1], padding='SAME') + self.b5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_relu5 = tf.nn.relu(h_conv5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool5 = tf.nn.max_pool3d(h_relu5, ksize=[1, 1, 1, 1, 1], strides=[1, 1, 1, 1, 1], padding='SAME')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_pool2_flat = tf.reshape(h_pool5, [-1, 10 * 10 * 10 * 60])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, self.fc1w) + self.fc1b)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, self.fc2w) + self.fc2b)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        y_conv = tf.matmul(h_fc2, self.fc3w) + self.fc3b123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "shape of y_conv:",  y_conv.get_shape()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return y_conv123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass AtomicNetworks(object):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    _single_ex_pipeline = None123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    _batch_pipeline = None123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    compute_output = None123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def __init__(self,b_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b_size = b_size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if self._single_ex_pipeline == None:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            raise ValueError('Single example pipeline needs to be defined in atomic networks.')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if self._batch_pipeline == None:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            raise ValueError('Batch pipeline needs to be defined in atomic networks.')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if self.compute_output == None:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            raise ValueError('Function compute_output() function needs to be defined in networks.')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _make_batch(self, input_q):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # create a self.b_size number of replicas of input pipe to process every example independently123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :param input_q:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :return:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b_net_feats = []123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b_transit_pars = []123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for i in range(self.b_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            net_feats, transit_pars = self._single_ex_pipeline(input_q=input_q)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if not isinstance(net_feats, list):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                raise Exception("Network output features from single example pipeline must be in a list")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            if i == 0:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                [b_net_feats.append([]) for i in range(len(net_feats))]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                [b_transit_pars.append([]) for i in range(len(transit_pars))]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            [b_net_feats[i].append(net_feats[i]) for i in range(len(net_feats))]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            [b_transit_pars[i].append(transit_pars[i]) for i in range(len(transit_pars))]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b_net_feats = [tf.stack(b_net_feats[i]) for i in range(len(b_net_feats))]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b_transit_pars = [tf.stack(b_transit_pars[i]) for i in range(len(b_transit_pars))]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return b_net_feats, b_transit_pars123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # TODO: protection for the radii (maxradii in config)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # TODO: require list ?123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass ConcatNet(AtomicNetworks):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def __init__(self,b_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # variables for concat Graham convolution 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.k_size_1 = [11,11,11]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.pix_size_1 = 0.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.conc_ll_w1 = weight_variable([11 * 11 * 11, 2, 200])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.conc_lr_w1 = weight_variable([11 * 11 * 11, 2, 200])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.conc_rr_w1 = weight_variable([11 * 11 * 11, 2, 100])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.conc_rl_w1 = weight_variable([11 * 11 * 11, 2, 100])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.conc_ll_b1 = bias_variable([200])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.conc_lr_b1 = bias_variable([200])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.conc_rr_b1 = bias_variable([100])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.conc_rl_b1 = bias_variable([100])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1_w = weight_variable([300, 256])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1_b = bias_variable([256])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc2_w = weight_variable([256, 2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc2_b = bias_variable([2])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.b_size = b_size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        super(ConcatNet, self).__init__(self.b_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _single_ex_pipeline(self,input_q):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :param input_q:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :param keep_prob:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :return:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        input_stream = input_q.dequeue()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_elem = input_stream[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_coord = input_stream[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_elem = input_stream[2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_coord = input_stream[3]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        ll_pairs = input_stream[4]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        ll_rel_coords = input_stream[5]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lr_pairs = input_stream[6]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lr_rel_coords = input_stream[7]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rr_pairs = input_stream[8]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rr_rel_coords = input_stream[9]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rl_pairs = input_stream[10]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rl_rel_coords = input_stream[11]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        transit_pars = input_stream[12:]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        l_feat_0 = tf.to_float(tf.expand_dims(lig_elem, 1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        r_feat_0 = tf.to_float(tf.expand_dims(rec_elem, 1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        l_feat_1, r_feat_1 = affinity.nn.concat_nonlinear_conv3d(k_size=self.k_size_1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 pix_size=self.pix_size_1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 s_feat=l_feat_0,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 d_feat=r_feat_0,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 ss_pairs=ll_pairs,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 sd_pairs=lr_pairs,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 dd_pairs=rr_pairs,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 ds_pairs=rl_pairs,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 ss_rel_coords=ll_rel_coords,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 sd_rel_coords=lr_rel_coords,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 dd_rel_coords=rr_rel_coords,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 ds_rel_coords=rl_rel_coords,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 ss_w=self.conc_ll_w1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 sd_w=self.conc_lr_w1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 dd_w=self.conc_rr_w1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 ds_w=self.conc_rl_w1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 ss_b=self.conc_ll_b1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 sd_b=self.conc_lr_w1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 dd_b=self.conc_rr_b1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                 ds_b=self.conc_rl_b1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        l_act_1 = tf.nn.relu(l_feat_1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        r_act_1 = tf.nn.relu(r_feat_1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        l_atomsum = tf.reduce_sum(l_act_1, reduction_indices=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        r_atomsum = tf.reduce_sum(r_act_1, reduction_indices=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return [l_atomsum,r_atomsum],transit_pars123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _batch_pipeline(self,b_feat):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :param b_feat:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        :return:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1 = tf.nn.relu(tf.matmul(b_feat, self.fc1_w) + self.fc1_b)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc1_drop = tf.nn.dropout(h_fc1,self.keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        h_fc2 = tf.matmul(h_fc1, self.fc2_w) + self.fc2_b123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return h_fc2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def compute_output(self,input_q,keep_prob):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.keep_prob = self.keep_prob123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b_net_feats,b_transit_pars = self._make_batch(input_q)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b_l_atomsum = b_net_feats[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b_r_atomsum = b_net_feats[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b_atomsum = tf.concat([b_l_atomsum,b_r_atomsum],1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        b_logits = self._batch_pipeline(b_atomsum)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return b_logits,b_transit_pars123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass AtomicNet(AtomicNetworks):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def __init__(self, b_size, keep_prob):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.batch_size = b_size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.keep_prob = keep_prob123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.atom_types = 9123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.radial_filters = 5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.filter_means = tf.Variable(tf.random_normal(shape=[ 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            (self.atom_types+1)**2, self.radial_filters], mean=3.0, stddev=1.5))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.filter_stds = tf.Variable(tf.constant(1.0,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            shape=[(self.atom_types+1)**2, self.radial_filters]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.radial_cutoff = 12 # maximum interaction distance123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.radial_scaling = 1.0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.radial_bias = 0.0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.hidden_units1 = 128123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1_w = tf.Variable(tf.random_normal(shape=[self.atom_types+1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.atom_types * self.radial_filters, self.hidden_units1], 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            mean=0.0, stddev=0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1_b = tf.Variable(tf.zeros([self.atom_types+1, self.hidden_units1]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.hidden_units2 = 128123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc2_w = tf.Variable(tf.random_normal(shape=[self.atom_types+1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.hidden_units1, self.hidden_units2], mean=0.0, stddev=0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc2_b = tf.Variable(tf.zeros([self.atom_types+1, self.hidden_units2]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.hidden_units3 = 64123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc3_w = tf.Variable(tf.random_normal(shape=[self.atom_types+1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.hidden_units2, self.hidden_units3], mean=0.0, stddev=0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc3_b = tf.Variable(tf.zeros([self.atom_types+1, self.hidden_units3]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.out_w = tf.Variable(tf.random_normal(shape=[self.atom_types+1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.hidden_units3, 1], mean=0.0, stddev=0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.out_b = tf.Variable(tf.zeros([self.atom_types+1, 1]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        super(AtomicNet, self).__init__(self.batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _single_ex_pipeline(self, input_q):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        input_stream = input_q.dequeue()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_coords = input_stream[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_nbr_idx = input_stream[1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_nbr_atoms = input_stream[2]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_elem = input_stream[3]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_coords = input_stream[4]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_nbr_idx = input_stream[5]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_nbr_atoms = input_stream[6]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_elem = input_stream[7]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        comp_coords = input_stream[8]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        comp_nbr_idx = input_stream[9]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        comp_nbr_atoms = input_stream[10]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        comp_elem = input_stream[11]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        transit_pars = input_stream[12:] #12: epoch, 13: label123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def compute_energy(coords, nbr_idx, nbr_atoms, source_atoms, keep_prob):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            nbr_atoms_mask = nbr_atoms > 0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            feature_matrix = affinity.nn.atomic_convolution_layer(self.filter_means,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                self.filter_stds, self.radial_filters, coords, source_atoms, 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                nbr_idx, nbr_atoms, nbr_atoms_mask, atom_types=self.atom_types,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                batch_size=1, radial_cutoff=self.radial_cutoff, neighbors=12, 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                radial_scaling=self.radial_scaling, radial_bias=self.radial_bias)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            feature_matrix = affinity.nn.atomistic_fc_layer(self.fc1_w, self.fc1_b,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                feature_matrix, source_atoms, self.atom_types*self.radial_filters,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                self.hidden_units1, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            feature_matrix = affinity.nn.atomistic_fc_layer(self.fc2_w, self.fc2_b,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                feature_matrix, source_atoms, self.hidden_units1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                self.hidden_units2, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            feature_matrix = affinity.nn.atomistic_fc_layer(self.fc3_w, self.fc3_b,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                feature_matrix, source_atoms, self.hidden_units2,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                self.hidden_units3, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            molecule_energy = affinity.nn.atomistic_output_layer(self.out_w, 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                self.out_b, feature_matrix, source_atoms)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return molecule_energy123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_energy = compute_energy(lig_coords, lig_nbr_idx, lig_nbr_atoms, lig_elem, self.keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_energy = compute_energy(rec_coords, rec_nbr_idx, rec_nbr_atoms, rec_elem, self.keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        comp_energy = compute_energy(comp_coords, comp_nbr_idx, comp_nbr_atoms, comp_elem, self.keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        change_energy = comp_energy - lig_energy - rec_energy123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return [change_energy], transit_pars123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def _batch_pipeline(self, b_feat):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return b_feat123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def compute_output(self, input_q):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        predicted_energies, b_transit_pars = self._make_batch(input_q)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        predicted_energies = self._batch_pipeline(predicted_energies)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return predicted_energies, b_transit_pars123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF"""TODO: Fix this - it still uses old/depreciated/nonexistent methods and format"""123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass VijayNet:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def __init__(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.atom_types = 7123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.radial_filters1 = 5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.filter_means1 = tf.Variable(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            tf.random_normal(shape=[(self.atom_types + 1) ** 2, self.radial_filters1], mean=3.0, stddev=1.5))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.filter_stds1 = tf.Variable(tf.constant(1.0, shape=[(self.atom_types + 1) ** 2, self.radial_filters1]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # TODO: add more radial filters123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.hidden_units1 = 128123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1_w = tf.Variable(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            tf.random_normal(shape=[self.atom_types + 1, self.atom_types * self.radial_filters1, self.hidden_units1],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                             mean=0.0, stddev=0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.fc1_b = tf.Variable(tf.zeros([self.atom_types + 1, self.hidden_units1]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # TODO: add more bias variables123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.out_w = tf.Variable(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            tf.random_normal(shape=[self.atom_types + 1, self.hidden_units1], mean=0.0, stddev=0.1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.out_b = tf.Variable(tf.zeros([self.atom_types + 1, self.hidden_units1]))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def compute_output(self, keep_prob, batch_size, ligand_coords, ligand_idx, ligand_atoms, receptor_coords,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                       receptor_idx, receptor_atoms, complex_coords, complex_idx, complex_atoms):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def compute_energy(coords, nbr_idx, nbr_atoms, keep_prob):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            source_atoms = tf.squeeze(nbr_atoms[:, :, 0])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            nbr_idx = nbr_idx[:, :, 1:]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            nbr_atoms = nbr_atoms[:, :, 1:]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            nbr_atoms_mask = nbr_atoms > 0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            feature_matrix = af.nn.atomic_convolution_layer(self.filter_means1, self.filter_stds1, self.radial_filters1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                            coords, source_atoms, nbr_idx, nbr_atoms, nbr_atoms_mask)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # TODO: Add more convolutional layers123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            feature_matrix = af.nn.vijay_fc_layer(self.fc1_w, self.fc1_b, feature_matrix, source_atoms,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                  input_dim=self.atom_types * self.radial_filters1,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                  output_dim=self.hidden_units1, keep_prob=keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # TODO: Add more fully-connected layers123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            molecule_energy = af.nn.vijay_output_layer(self.out_w, self.out_b, feature_matrix, source_atoms)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return molecule_energy123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        ligand_energy = compute_energy(ligand_coords, ligand_idx, ligand_atoms, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        receptor_energy = compute_energy(receptor_coords, receptor_idx, receptor_atoms, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        complex_energy = compute_energy(complex_coords, complex_idx, complex_atoms, keep_prob)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        change_energy = complex_energy - ligand_energy - receptor_energy123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return change_energy