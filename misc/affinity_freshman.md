###Quick Introduction to Affinity  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF_The aim of the resources on this page is to allow anyone, even without specific machine learning background, to quickly get up to speeed with Affinity Core virtual screening engine. The estimated time for completions is under two weeks._123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF####Background Readings123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF_If you are familiar with all of the concepts the list: weights, biases, activation function, ReLU, softmax, convolution, pooling, layers of depth, batch, gradient descent, backpropagation (and chain rule), AdamOptimizer, please feel free to skip to the next section._  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFCS231n "Convolutional Neural Networks for Visual Recognition"  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFhttp://cs231n.github.io/  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFPlease, read through  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFModule 1, Neural Networks      123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFModule 2, Convolutional Neural Networks   123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF####TensorFlow tutorials to complete123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF_If you are familiar with all of the concepts in this list: tensor graph, session, tf.global_variable_initializer, tf.train.coordinator, tf.train.start_queue_runners, tf.nn.sparse_softmax_cross_entropy_with_logits, tf.saver, tf.summary, tf.summary.FileWriter, tf.name_scope, please, feel free to skip to the next section._123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF[MNIST](https://www.tensorflow.org/tutorials/mnist/beginners/)  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF[Deep MNIST](https://www.tensorflow.org/tutorials/mnist/pros/)  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF[Understandig TensorFlow's workflow](https://www.tensorflow.org/tutorials/mnist/tf/)  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF[CIFAR10](https://www.tensorflow.org/tutorials/deep_cnn/)  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF[Deep Generative Adversarial Models](https://github.com/carpedm20/DCGAN-tensorflow)  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF####Introduction to Affinity Core123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF_Structure based virtual screening is an approach that allows to retrieve a very small percent, usually few dozens of molecules, from the large database, of millioons of chemical structures. The process can be imagined as a google search for a flexible key (ligand) with a 3D image of a rigid lock (receptor,protein). Search can be broken into two parts. Since the most optimal relative position of the drug and protein is not known, it has to be estimated (docking). Afterwards, many static protein-ligand complexes have to be ranked by their predicted relative binding affinity (sorting). Usually, 25,000-200,000 pose evaluations are done during docking, and a single pose evaluation is done during ranking. Because Tesla K80 GPU can only evaluate 100-200 images/second, position search for a single ligand may take anywhere between 3 and 35 minutes, docking the average-size database of 1,000,000 of molecules may take 1.2 GPU years. In this example we only apply the network to the previously docked with AutoDock Smina positions, IE: ranking._123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF####Step 1: teaching the network123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFYou will need four scripts, and the database123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF```123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFav4_networks.py123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFav4_main.py123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFav4_input.py123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFav4_utils.py123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFlabeled_av4123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF```123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFwhere `labeled_av4` is an already prepared database of the ligands and proteins in av4 binary format.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF```123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFav4_networks.py 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# is a library of many different network architectures123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# each of the networks accepts batch of images, and outputs batch of unscaled probabilities (logits)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# crucial part 1: the network itself123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# convolutional layers IE: tf.nn.conv3d123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# pooling layers IE: tf.nn.max_pool3d or tf.nn.avg_pool3d123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# Rectifier Linear Regression Units, or ReLUs IE: tf.nn.relu123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# crucial part 2: rules for variable initialization123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# IE: bias_variable 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# tf.constant(0.01, shape=shape)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# or weight variable123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# tf.truncated_normal(shape, stddev=0.005)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# initial weights and biases for trainable variables are usually initialized with small random positive 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# values. Deep networks can easily run out of control and owerflow the floats in higher layers 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# if not initialized properly.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# That's why it's important to initialize variables very accurately123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# especially deep networks can be hierarchically constructed 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# when the new layer(s) is added to the top of existing trained network123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# crucial part 3: variable summaries123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# IE: tf.summary.histogram, tf.summary.scalar, 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# and tf.name_scope (groups variables together under a common name)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# variable summaries are written in a separate file and help to monitor the state and evolution of the network123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# during training or testing123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFav4_input.py123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# is script that reads and indexes the database in av4 format, and creates batches of images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# in 3D that can be fed to the network123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# av4 database consists of thousands of folders - one for each protein123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# each protein can have many ligands123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# each ligand av4 file can have many positions (frames), and every frame has it's label123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# the name of each folder IE 1QGT, 4G93 each correspond to a particular PDB id original file for which can be 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# found at http://www.rcsb.org/123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# crucial part 1: index_the_database_into_queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# crawls of all the folders in the database and creates tensor of filenames123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# crucial part 2: read_receptor_and_ligand123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# reads a single example of receptor and ligand from the database123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# returns coordinates and name (label) of every atom (only one frame depending on epoch counter)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# crucial part 3: convert_protein_and_ligand_to_image123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# creates an image (sparse or dense) from input atom coordinates123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# dense image is cubic, has only some atoms of the protein, and describes pixels (not atoms)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# empty spaces in dense image are filled with zeros123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# crucial part 4: tf.train.batch (for three reasons)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# reason 1:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# it runs multiple independent threads of image creation123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# reason 2:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# it batches images together123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# single images are small and tensor operations in them (such as convolution or pooling)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# are not efficient. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# reason 3:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# gradient descent optimizer gets much better gradient from multiple images. 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# Ideally, every single gradient descent step would be applied to a representative sample 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# of a whole database; this is better achievable with larger batches123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# av4_main.py123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF``` 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFa library of different networks  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFall of the networks accept  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFkeeps together hyperparameters of the model such as: batch size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF####Step 2: evaluating the network123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFav4_eval123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#####Step 3: database preparation (optional)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdata and .av4 format123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFav4_database_master123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFav4_atom_dictionary123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFor development purposes we host an AWS instance with a single Tesla K80 GPU123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF`#clone affinity core into your working directory 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFubuntu@ip-172-31-4-5:~/maksym$ git clone https://github.com/mitaffinity/core.git  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFcd core`  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFpython av4_main.py  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFvi 123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#To change the database path under flags to   123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#/home/ubuntu/common/data/labeled_av4  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFDoes not work needs latest tensorflow  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFSource $TF12  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIf you are interested what it is:  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF/home/ubuntu/common/venv/tf12/bin/activate  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe training should start now  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFpython av4_main.py  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFThe process will break when one exits the terminal  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFLaunch on the background  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFpython av4_main.py &  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFBackground process will persist regardless if you are in ssh session or not  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFOnly one person can use the GPU with TF at the same time by default.  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFTo see if anything is running  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFnvidia-smi  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFtop  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFDevelopment zone kill processes with  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFpkill -9 python  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFUnderstanding the outputs:  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1_logs   123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFUsually evaluations are written here  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1_netstate saves the trained weights of the network  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1_test saves visualization of testing  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF1_train saves visualization of train  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFVisualizing the network  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFsudo python -m tensorflow.tensorboard --logdir=. --port=80  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF`123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFOpen  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFhttp://awsinstance.com/  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIn your browser to visualize the network. This thing can crawl all the directories  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFHeavy lifting  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFClusters  123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF