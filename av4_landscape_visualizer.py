import time,re123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4_input import image_and_label_queue,index_the_database_into_queue,read_receptor_and_ligand,convert_protein_and_ligand_to_image123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4_main import FLAGS123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4_networks import max_net123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4_utils import generate_exhaustive_affine_transform,deep_affine_transform,affine_transform,generate_identity_matrices123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.saved_session = './summaries/4_netstate/saved_state-82099'123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.predictions_file_path = re.sub("netstate","logs",FLAGS.saved_session)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.database_path = '../datasets/holdout_av4'123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.num_epochs = 2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFFLAGS.top_k = FLAGS.num_epochs123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# av4 lanscape visualizer makes visualizaition of the predicted energy landscape in VMD123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# take a single ligand;123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# generate affine transform123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# enqueue all of the ligands into a queue without shuffling123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# take the ligands ; collect the energy terms123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass minima_search_agent:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    class evaluations_container:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """" groups together information about the evaluated positions in a memory-efficient way.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        ligand pose transformations stores (groups of) affine transformation matrices """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        labels = np.array([])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        predictions = np.array([])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        ligand_pose_transformations = np.array([]).reshape([0,4,4])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        cameraviews = np.array([]).reshape([0,4,4])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def add_batch(self,prediction_batch,ligand_pose_transformation_batch,cameraview_batch,label_batch=[]):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            "adds batch of predictions for different positions and cameraviews of the ligand"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.labels = np.append(self.labels,label_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.predictions = np.append(self.predictions,prediction_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.ligand_pose_transformations = np.append(self.ligand_pose_transformations,ligand_pose_transformation_batch,axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.cameraviews = np.append(self.cameraviews,cameraview_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return len(self.predictions)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#        # example 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#        # take all (do not remove middle) dig a hole in the landscape123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#        # batch is a training example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def convert_into_training_batches(self,positives_in_batch=10,negatives_in_batch=90,num_batches=10000):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """takes predictions for which the label * prediction is the highest123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            (label * prediction is equal to the cost to the network)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            constructs num_batches of bathes with a single positive example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            returns a list of batches123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # TODO: variance option for positives and negatives123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # TODO: clustering123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # TODO: add VDW possibilities123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # TODO: RMSD + the fraction of native contacts123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # generate labels first123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # in this case positives labels are simply the examples with identity affine transform123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # everything that is not the initial conformation is called negatives123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.labels = np.asarray(map(lambda affine_transform: np.all(affine_transform == np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])),self.ligand_pose_transformations))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # sort in descending order by predictions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            order = np.argsort(self.predictions)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "order:",order123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "labels:",self.labels123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.labels = self.labels[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "reordered labels:",self.labels123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.predictions = self.predictions[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.ligand_pose_transformations = self.ligand_pose_transformations[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.cameraviews = self.cameraviews[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # take hardest positive and negative labels;123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            positives_idx = (np.arange(len(self.labels))[self.labels])[-positives_in_batch:]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            negatives_idx = (np.arange(len(self.labels))[-self.labels])[:negatives_in_batch]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "positives idx:", positives_idx123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "negatives idx:", negatives_idx123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "labels:", self.labels123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "all positives:", np.arange(len(self.labels))[self.labels], len(np.arange(len(self.labels))[self.labels])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "all negatives:", np.arange(len(self.labels))[-self.labels], len(np.arange(len(self.labels))[-self.labels])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #batch_positives_idx = positives_idx[-(1+np.arange(positives_in_batch)+ i*positives_in_batch)]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #batch_negatives_idx = negatives_idx[np.arange(negatives_in_batch)+i*negatives_in_batch]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #batch_idx = np.concatenate([batch_positives_idx,batch_negatives_idx],axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #print "positives:",positives_idx,len(positives_idx)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #print "negatives:",negatives_idx,len(negatives_idx)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #print "batch positives:", batch_positives_idx123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #print "batch negatives:", batch_negatives_idx123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #print "predictions for correct pose:", self.predictions[identity_idx],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #print "min:",np.min(self.predictions[identity_idx]),"max:",np.max(self.predictions[identity_idx])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #print "correct pose rank:", np.sum(np.asarray((self.predictions[-identity_idx] > np.average(self.predictions[identity_idx])),dtype=np.int32))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            # TODO(maksym): it can be an interesting idea to cluster the results for training123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#        # example 2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#        # remove semi-correct conformations123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#        # batch is a training example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#        def convert_into_training_batches(self,batch_size,num_batches=2):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            return 999123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       # example 3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       # remove semi-correct conformations123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       # take many images of positive123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#       # image is a training example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def __init__(self,side_pixels=FLAGS.side_pixels,pixel_size=FLAGS.pixel_size,batch_size=FLAGS.batch_size,num_threads=FLAGS.num_threads,sess = FLAGS.main_session):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # generate a single image from protein coordinates, and ligand coordinates + transition matrix123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess = sess123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.ligand_pose_transformations_queue = tf.FIFOQueue(capacity=80000,dtypes=tf.float32,shapes=[4,4])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.ligand_pose_transformations = tf.concat([generate_identity_matrices(num_frames=1000),generate_exhaustive_affine_transform()],0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.ligand_elements = tf.Variable([0],trainable=False,validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.ligand_coords = tf.Variable([[0.0,0.0,0.0]],trainable=False,validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.receptor_elements = tf.Variable([0],trainable=False,validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.receptor_coords = tf.Variable([[0.0,0.0,0.0]],trainable=False,validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        transformed_ligand_coords,ligand_pose_transformation = affine_transform(self.ligand_coords,self.ligand_pose_transformations_queue.dequeue())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # TODO: create fast reject for overlapping atoms of the protein and ligand123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        complex_image,_,cameraview = convert_protein_and_ligand_to_image(self.ligand_elements,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                     transformed_ligand_coords,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                     self.receptor_elements, self.receptor_coords,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                     side_pixels, pixel_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # create and enqueue images in many threads, and deque and score images in a main thread123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord = tf.train.Coordinator()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.minimizer_queue = tf.FIFOQueue(capacity=batch_size*5,dtypes=[tf.float32,tf.float32,tf.float32],shapes=[[side_pixels,side_pixels,side_pixels],[4,4],[4,4]])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.minimizer_queue_enqueue = self.minimizer_queue.enqueue([complex_image,ligand_pose_transformation,cameraview])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.queue_runner = tf.train.QueueRunner(self.minimizer_queue,[self.minimizer_queue_enqueue]*num_threads)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.image_batch,self.ligand_pose_transformation_batch,self.cameraview_batch = self.minimizer_queue.dequeue_many(batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.keep_prob = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        with tf.name_scope("network"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            y_conv = max_net(self.image_batch, self.keep_prob, batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.predictions_batch = tf.nn.softmax(y_conv)[:,1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # create a pipeline to convert ligand transition matrices + cameraviews into images again123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def evaluate_all_positions(self,my_ligand_elements,my_ligand_coords,my_receptor_elements,my_receptor_coords):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(self.ligand_pose_transformations_queue.enqueue_many(self.ligand_pose_transformations))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "enqueue affine transform:", time.sleep(1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.enqueue_threads = self.queue_runner.create_threads(self.sess, coord=self.coord, start=False, daemon=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "launch threads:",time.sleep(1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(tf.assign(self.ligand_elements,my_ligand_elements,validate_shape=False))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(tf.assign(self.ligand_coords,my_ligand_coords,validate_shape=False))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(tf.assign(self.receptor_elements,my_receptor_elements,validate_shape=False))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(tf.assign(self.receptor_coords,my_receptor_coords,validate_shape=False))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # re-initialize the evalutions class123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.evaluated = self.evaluations_container()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "print queue size -3:", self.sess.run(self.minimizer_queue.size())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for tr in self.enqueue_threads:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            tr.start()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "print queue size -2:", self.sess.run(self.minimizer_queue.size())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        time.sleep(0.5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "print queue size -1:", self.sess.run(self.minimizer_queue.size())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "start evaluations...."123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        time.sleep(0.5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        try:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            while True:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                start = time.time()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                my_prediction_batch,my_image_batch,my_ligand_pose_transformation_batch,my_cameraview_batch = \123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    self.sess.run([self.predictions_batch,self.image_batch,self.ligand_pose_transformation_batch,self.cameraview_batch],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                  feed_dict = {self.keep_prob:1},options=tf.RunOptions(timeout_in_ms=1000))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                # save the predictions and cameraviews into the memory123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                positions_evaluated = self.evaluated.add_batch(my_prediction_batch,my_ligand_pose_transformation_batch,my_cameraview_batch)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                print "ligand_atoms:",np.sum(np.array(my_image_batch >7,dtype=np.int32)),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                print "\tpositions evaluated:",positions_evaluated,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                print "\texamples per second:", "%.2f" % (FLAGS.batch_size / (time.time() - start))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        except tf.errors.DeadlineExceededError:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # create training examples for the main queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.evaluated.convert_into_training_batches(positives_in_batch=20,negatives_in_batch=200,num_batches=5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # empty the queue and continue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            try:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                while True:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    print self.sess.run(self.minimizer_queue.dequeue(),options=tf.RunOptions(timeout_in_ms=1000))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            except tf.errors.DeadlineExceededError:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                print "evaluations finished; moving to the next protein"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return 0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFminimizer1 = minima_search_agent()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef evaluate_on_train_set():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    "train a network"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.name_scope("epoch_counter"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        batch_counter = tf.Variable(0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        batch_counter_increment = tf.assign(batch_counter,tf.Variable(0).count_up_to(np.round((100000*FLAGS.num_epochs)/FLAGS.batch_size)))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        epoch_counter = tf.div(batch_counter*FLAGS.batch_size,1000000)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create session to compute evaluation123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    sess = FLAGS.main_session123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create a filename queue first123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    filename_queue,examples_in_database = index_the_database_into_queue(FLAGS.database_path, shuffle=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create an epoch counter123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # there is an additional step with variable initialization in order to get the name of "count up to" in the graph123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    batch_counter = tf.Variable(0,trainable=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # read one receptor and stack of ligands; choose one of the ligands from the stack according to epoch123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    ligand_file,current_epoch,label,ligand_elements,ligand_coords,receptor_elements,receptor_coords = read_receptor_and_ligand(filename_queue,epoch_counter=tf.constant(0))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create saver to save and load the network state123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    sess.run(tf.global_variables_initializer())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    saver= tf.train.Saver(var_list=(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="Adam_optimizer") +123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                             tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="network") +123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                             tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="epoch_counter")))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    if FLAGS.saved_session is None:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        sess.run(tf.global_variables_initializer())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        sess.run(tf.global_variables_initializer())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "Restoring variables from sleep. This may take a while..."123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        saver.restore(sess,FLAGS.saved_session)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "unitialized vars:", sess.run(tf.report_uninitialized_variables())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        time.sleep(1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    coord = tf.train.Coordinator()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    threads = tf.train.start_queue_runners(sess = sess,coord=coord)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    batch_num = 0123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    while True or not coord.should_stop():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        start = time.time()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "print taking next ligand:", sess.run(ligand_file)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        my_ligand_file,my_current_epoch,my_label,my_ligand_elements,my_ligand_coords,my_receptor_elements,my_receptor_coords = \123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            sess.run([ligand_file,current_epoch,label,ligand_elements,ligand_coords,receptor_elements,receptor_coords])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "obtained coordinates trying to minimize the ligand "123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        minimizer1.evaluate_all_positions(my_ligand_elements,my_ligand_coords,my_receptor_elements,my_receptor_coords)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # minimizer should return many images123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # for simplicity of visualization of what is going on, the images will be in python (and for error notifications)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # images can be enqueud into the main queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "batch_num:",batch_num123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "\texamples per second:", "%.2f" % (FLAGS.batch_size / (time.time() - start))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFevaluate_on_train_set()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFprint "All Done"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #self.sess.run(self.minimizer_queue.close(cancel_pending_enqueues=True))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #self.coord._stop_event.set()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #self.coord.clear_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            for tr in self.enqueue_threads:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#                print tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#                tr.stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            self.coord.clear_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            print "stop has been cleared"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            self.sess.run(self.affine_transform_queue.enqueue_many(self.exhaustive_affine_transform))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            self.minimizer_queue_enqueue = self.minimizer_queue.enqueue(self.complex_image)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            self.queue_runner = tf.train.QueueRunner(self.minimizer_queue, [self.minimizer_queue_enqueue] * 8)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF #           self.enqueue_threads = self.queue_runner.create_threads(self.sess, coord=self.coord, start=False,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF #                                                                   daemon=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            print "print queue size 1:", self.sess.run(self.minimizer_queue.size())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            print "starting enqueue threads again"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF #           for tr in self.enqueue_threads:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF #               print tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF #              tr.start()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF  #          print "enqueue threads have started again"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF   #         print "queue size 2:", self.sess.run(self.minimizer_queue.size())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF   #         time.sleep(5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF   #         print "queue size 3", self.sess.run(self.minimizer_queue.size())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF   #         time.sleep(2)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            #self.sess.run(self.image_batch, options=tf.RunOptions(timeout_in_ms=1000))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            time.sleep(1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            self.coord.join(threads=self.enqueue_threads,stop_grace_period_secs=5)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            # Clearning stop !123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            print "joined threads:", time.sleep(1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            self.sess.run(self.image_batch, options=tf.RunOptions(timeout_in_ms=1000))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF #           print "one more image batch done!",time.sleep(1)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            print "evaluation finished"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#            return 0