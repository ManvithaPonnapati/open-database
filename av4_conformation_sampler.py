import time123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport av4_input123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom av4_config import FLAGS123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport av4_networks123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport av4_utils123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef softmax_cross_entropy_with_RMSD(logits,lig_RMSDs,RMSD_threshold=3.0):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Calculates usual sparse softmax cross entropy for two class classification between 1(correct position)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    and 0(incorrect position) and multiplies the resulting cross entropy by RMSD coefficient.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    In this implementation every other position except initial is considered to be incorrect, or 0.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    | RMSD_ligand > RMSD_threshold | RMSDcoeff = 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    | RMSD_ligand < RMSD_threshold | RMSDcoeff = (RMSD_threshold - RMSD_ligand)/RMSD_threshold123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    RMSD threshold is in Angstroms.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    labels = tf.cast((lig_RMSDs < 0.01), tf.int32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    cost_correct_positions = cross_entropy * tf.cast(labels,tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    cost_incorrect_positions = cross_entropy * tf.cast((lig_RMSDs > RMSD_threshold), tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    cost_semicorrect_positions = cross_entropy \123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                 * tf.cast((lig_RMSDs < RMSD_threshold), tf.float32) \123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                 * tf.cast((lig_RMSDs > 0.01), tf.float32) * (lig_RMSDs/RMSD_threshold)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return cost_incorrect_positions + cost_semicorrect_positions + cost_correct_positions123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFclass SamplingAgent:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """ Sampling Agent takes a single protein with a bound ligand and 1) samples many possible protein-ligand conformations,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    2) samples many many camera views of the correct position 3) Outputs a single batch of images for which the123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    gradient is highest.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: variance option for positives and negatives123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: clustering123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: add VDW possibilities123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: RMSD + the fraction of native contacts123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: add fast reject for overlapping atoms123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: in the future be sure not to find poses for a similar ligand123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: leave a single binding site on the level of data preparation123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: create fast reject for overlapping atoms of the protein and ligand123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # example 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # take all (do not remove middle) dig a hole in the landscape123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # batch is a training example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # needs labels123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # example 2123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # remove semi-correct conformations123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #        # batch is a training example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #       # example 3123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #       # remove semi-correct conformations123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #       # take many images of positive123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #       # image is a training example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    class EvaluationsContainer:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """" Groups together information about the evaluated positions in a form of affine transform matrices. Reduces123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        all of the evaluated poses into training batch.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # TODO: add a possibility to generate new matrices based on performed evaluations.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # (aka genetic search in AutoDock)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def __init__(self):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.preds = np.array([])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.costs = np.array([])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_pose_transforms = np.array([]).reshape([0, 4, 4])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.cameraviews = np.array([]).reshape([0, 4, 4])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_RMSDs = np.array([])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def add_batch(self, pred_batch, cost_batch, lig_pose_transform_batch, cameraview_batch, lig_RMSD_batch):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """ Adds batch of predictions for different positions and cameraviews of the ligand.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.preds = np.append(self.preds, pred_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.costs = np.append(self.costs,cost_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_pose_transforms = np.append(self.lig_pose_transforms, lig_pose_transform_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.cameraviews = np.append(self.cameraviews, cameraview_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_RMSDs = np.append(self.lig_RMSDs, lig_RMSD_batch, axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return len(self.preds)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        def convert_into_training_batch(self, cameraviews_initial_pose=10, generated_poses=90, remember_poses=300):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """ Returns examples with a highest cost/gradient.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            [cameraviews_initial_pose] + [generated_poses] should be == to [desired batch size] for training123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # order all parameters by costs in ascending order123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            order = np.argsort(self.costs)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.preds = self.preds[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.costs = self.costs[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_pose_transforms = self.lig_pose_transforms[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.cameraviews = self.cameraviews[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            self.lig_RMSDs = self.lig_RMSDs[order]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # Take examples for which the cost is highest.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # The arbitrary number of RMSD of 0.01 which distinguishes correct from incorrect examples should not123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # affect the results at all. Positions with small ligand_RMSDs < RMSD_threshold will only become training123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # examples after no (ligand_RMSD > RMSD_threshold) is left. It is same with sliding along the threshold.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            init_poses_idx = (np.where(self.lig_RMSDs < 0.01)[0])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            gen_poses_idx = (np.where(self.lig_RMSDs > 0.01)[0])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            sel_init_poses_idx = init_poses_idx[-cameraviews_initial_pose:]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            sel_gen_poses_idx = gen_poses_idx[-generated_poses:]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # print a lot of statistics for debugging/monitoring purposes123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # todo: print to file with agent name123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # print "statistics sampled conformations:"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # var_list = {'lig_RMSDs':self.lig_RMSDs,'preds':self.preds,'costs':self.costs}123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # av4_utils.describe_variables(var_list)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # print "statistics for selected (hardest) initial conformations"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # var_list = {'lig_RMSDs':self.lig_RMSDs[sel_init_poses_idx], 'preds':self.preds[sel_init_poses_idx],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #             'costs':self.costs[sel_init_poses_idx]}123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # av4_utils.describe_variables(var_list)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # print "statistics for selected (hardest) generated conformations"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # var_list = {'lig_RMSDs':self.lig_RMSDs[sel_gen_poses_idx], 'preds':self.preds[sel_gen_poses_idx],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            #             'costs':self.costs[sel_gen_poses_idx]}123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # av4_utils.describe_variables(var_list)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            sel_idx = np.hstack([sel_init_poses_idx,sel_gen_poses_idx])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            return self.lig_pose_transforms[sel_idx],self.cameraviews[sel_idx]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def __init__(self,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 agent_name,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 gpu_name,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 training_queue,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 side_pixels=FLAGS.side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 pixel_size=FLAGS.pixel_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 batch_size=FLAGS.batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 num_threads=FLAGS.num_threads,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 shift_ranges = FLAGS.shift_ranges,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 shift_deltas = FLAGS.shift_deltas,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 initial_pose_evals = FLAGS.initial_pose_evals,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 train_batch_init_poses = FLAGS.train_batch_init_poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 train_batch_gen_poses = FLAGS.train_batch_gen_poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                 sess = FLAGS.main_session):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Fill the queue with affine transformation matrices123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.agent_name = agent_name123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess = sess123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord = tf.train.Coordinator()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # -------------------- Set up affine transform queue with the ligand positions ------------------------------ #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_pose_tforms = tf.concat([av4_utils.generate_identity_matrices(initial_pose_evals),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                     av4_utils.generate_exhaustive_affine_transform(shift_ranges,shift_deltas)],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                    0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        affine_tforms_queue = tf.FIFOQueue(capacity=100000, dtypes=tf.float32,shapes=[4, 4])                                # todo capacity123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._affine_tforms_queue_clean = affine_tforms_queue.enqueue_many(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            [av4_utils.generate_identity_matrices(num_threads*3)])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._affine_tforms_queue_enq = affine_tforms_queue.enqueue_many(lig_pose_tforms)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # ------------------- Create assign options for receptor and ligand ----------------------------------------- #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_elem = tf.Variable([0],trainable=False, validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_coord = tf.Variable([[0.0,0.0,0.0]], trainable=False, validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_elem = tf.Variable([0],trainable=False, validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        rec_coord = tf.Variable([[0.0,0.0,0.0]], trainable=False, validate_shape=False)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._tform = affine_tforms_queue.dequeue()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        tformed_lig_coord,lig_pose_tform = av4_utils.affine_transform(lig_coord,self._tform)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._lig_elem_plc = tf.placeholder(tf.int32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._lig_coord_plc = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._rec_elem_plc = tf.placeholder(tf.int32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._rec_coord_plc = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._ass_lig_elem = tf.assign(lig_elem,self._lig_elem_plc, validate_shape=False, use_locking=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._ass_lig_coord = tf.assign(lig_coord, self._lig_coord_plc, validate_shape=False, use_locking=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._ass_rec_elem = tf.assign(rec_elem, self._rec_elem_plc, validate_shape=False, use_locking=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._ass_rec_coord = tf.assign(rec_coord, self._rec_coord_plc, validate_shape=False, use_locking=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # ------------------- Image creation pipeline --------------------------------------------------------------- #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        complex_image,_,cameraview = av4_input.convert_protein_and_ligand_to_image(lig_elem,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                   tformed_lig_coord,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                   rec_elem,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                   rec_coord,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                   side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                   pixel_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # calculate Root Mean Square Deviation for atoms of the transformed molecule compared to the initial one123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        lig_RMSD = tf.reduce_mean(tf.square(tformed_lig_coord - lig_coord))**0.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # create and enqueue images in many threads, and deque and score images in a main thread123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        image_queue = tf.FIFOQueue(capacity=batch_size*5,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                   dtypes=[tf.float32,tf.float32,tf.float32,tf.float32],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                   shapes=[[side_pixels,side_pixels,side_pixels], [4,4], [4,4], []])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._image_queue_deq = image_queue.dequeue()[0]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._image_queue_enq = image_queue.enqueue([complex_image, lig_pose_tform, cameraview, lig_RMSD])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._queue_runner = av4_utils.QueueRunner(image_queue, [self._image_queue_enq]*num_threads)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._image_batch, self._lig_pose_tform_batch, self._cameraview_batch, self._lig_RMSD_batch = \123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            image_queue.dequeue_many(batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # -------------------- Evaluation of the images with the network -------------------------------------------- #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._keep_prob = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if gpu_name is not None:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            with tf.device(gpu_name):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                with tf.name_scope("network"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                    y_conv = av4_networks.max_net.compute_output(self._image_batch, self._keep_prob, batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # calculate both predictions, and costs for every ligand position in the batch123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._pred_batch = tf.nn.softmax(y_conv)[:,1]123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._cost_batch = softmax_cross_entropy_with_RMSD(y_conv, self._lig_RMSD_batch)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # -------------------- Regeneration of the images from affine transform matrices for training --------------- #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # This part of the pipeline is for re-creation of already scored images from affine transform matrices123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # describing the conformation of ligand, and a particular cameraview from which the 3D snapshot was taken.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # storing affine transform matrices instead of images allows to save memory123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # create queue and enque pipe with two placeholders123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        r_tforms_and_cameraviews_queue = tf.FIFOQueue(capacity=train_batch_init_poses + train_batch_gen_poses,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                      dtypes=[tf.float32,tf.float32],shapes=[[4,4],[4,4]])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tforms_and_cameraviews_queue_clean = r_tforms_and_cameraviews_queue.enqueue_many(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            [av4_utils.generate_identity_matrices(num_threads*3),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF             av4_utils.generate_identity_matrices(num_threads*3)])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tforms_plc = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_cameraviews_plc = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tforms_cameraviews_enq = r_tforms_and_cameraviews_queue.enqueue_many([self._r_tforms_plc,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                      self._r_cameraviews_plc])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tform,self.r_cameraview = r_tforms_and_cameraviews_queue.dequeue()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_tformed_lig_coords,_ = av4_utils.affine_transform(lig_coord, self._r_tform)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self._r_complex_image,_,_ = av4_input.convert_protein_and_ligand_to_image(lig_elem,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                  self._r_tformed_lig_coords,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                  rec_elem,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                  rec_coord,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                  side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                  pixel_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                  self.r_cameraview)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # put images back to the image queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.r_lig_RMSD = tf.reduce_mean(tf.square(self._r_tformed_lig_coords - lig_coord))**0.5123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        r_image_queue_enq = image_queue.enqueue([self._r_complex_image,self._r_tform,self.r_cameraview,self.r_lig_RMSD])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.r_queue_runner = av4_utils.QueueRunner(image_queue, [r_image_queue_enq] * num_threads)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # enque to the training queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.pass_batch_to_the_training_queue = training_queue.enqueue_many([self._image_batch, self._lig_RMSD_batch])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    def grid_evaluate_positions(self,my_lig_elem,my_lig_coord,my_rec_elem,my_rec_coord):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """ Puts ligand in the center of every square of the box around the ligand, performs network evaluation of123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        every conformation.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Enqueue all of the transformations for the ligand to sample.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run([self._affine_tforms_queue_enq])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Assign elements and coordinates of protein and ligand; shape of the variable will change from ligand to ligand123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run([self._ass_lig_elem,self._ass_lig_coord,self._ass_rec_elem,self._ass_rec_coord],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                      feed_dict={self._lig_elem_plc:my_lig_elem,self._lig_coord_plc:my_lig_coord,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                 self._rec_elem_plc:my_rec_elem,self._rec_coord_plc:my_rec_coord})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #                                                                                                                  TODO: is there a guarantee that assignment completes before read123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # re-initialize the evalutions class123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        evaluated = self.EvaluationsContainer()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "shapes of the ligand and protein:", "unknown"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#        print self.sess.run([tf.shape(self.lig_elements),                                                                   # TODO: this may be useful here123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#                             tf.shape(self.lig_coords),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#                             tf.shape(self.rec_elements),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF#                            tf.shape(self.rec_coords)])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # start threads to fill the queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print "starting threads for the conformation sampler."123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.enqueue_threads = self._queue_runner.create_threads(self.sess, coord=self.coord, start=True, daemon=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        #self.sess.run(self.enqueue_threads_start)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for i in range(1):                                                                                                                # TODO controls123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            start = time.time()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            my_pred_batch, my_cost_batch, my_image_batch, my_lig_pose_tform_batch, my_cameraview_batch, my_lig_RMSD_batch = \123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                self.sess.run([self._pred_batch,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               self._cost_batch,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               self._image_batch,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               self._lig_pose_tform_batch,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               self._cameraview_batch,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                               self._lig_RMSD_batch],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                              feed_dict = {self._keep_prob:1})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            # save the predictions and cameraviews from the batch into evaluations container123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            lig_poses_evaluated = evaluated.add_batch(my_pred_batch,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                               my_cost_batch,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                               my_lig_pose_tform_batch,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                               my_cameraview_batch,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                               my_lig_RMSD_batch)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print self.agent_name,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "\tligand_atoms:",np.sum(np.array(my_image_batch >7,dtype=np.int32)),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "\tpositions evaluated:",lig_poses_evaluated,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print "\texamples per second:", "%.2f" % (FLAGS.batch_size / (time.time() - start))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # create training examples for the main queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        sel_lig_tforms,sel_cameraviews = evaluated.convert_into_training_batch(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            cameraviews_initial_pose=50,generated_poses=50,remember_poses=300)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # accurately terminate all threads without closing the queue (uses custom QueueRunner class)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.request_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(self._affine_tforms_queue_clean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.join()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.clear_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        av4_utils.dequeue_all(self.sess,self._tform)        # empty affine transform queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        av4_utils.dequeue_all(self.sess,self._image_queue_deq)         # empty image queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # regenerate a selected batch of images from ligand transformations and cameraviews123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # enqueue the Rregenerator123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run([self._r_tforms_cameraviews_enq],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                      feed_dict={self._r_tforms_plc: sel_lig_tforms,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                 self._r_cameraviews_plc: sel_cameraviews})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # start threads to fill the REGENERATOR queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.r_enqueue_threads = self.r_queue_runner.create_threads(self.sess, coord=self.coord, start=True, daemon=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(self.pass_batch_to_the_training_queue)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.request_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # accurately terminate all threads without closing the queue (uses custom QueueRunner class)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.sess.run(self._r_tforms_and_cameraviews_queue_clean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.join()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        self.coord.clear_stop()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        av4_utils.dequeue_all(self.sess,self._r_tform)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        av4_utils.dequeue_all(self.sess,self._image_queue_deq)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        return None123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF