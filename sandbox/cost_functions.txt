#cost functions 

#also try different weights
#try with different labels
#try training with different orders of functions

def mce(logits, labels):
	'''Minimal classification error: cost is the number of classification errors (because we have binary classes, it's either 0 or 1)
		mce = softmax(true_logits)
	'''
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_true_logits = tf.SparseTensor(indices=indices, values=logits,shape=[batch_size, num_classes])
    true_logits = tf.sparse_tensor_to_dense(sparse_true_logits)    
    opp_indices = tf.cast(tf.pack((tf.range(0, batch_size), -labels+1), axis=1), dtype=tf.int64)
    sparse_false_logits = tf.SparseTensor(indices=opp_indices, values=logits,shape=[batch_size, num_classes])
    false_logits = tf.sparse_tensor_to_dense(sparse_false_logits)    
    return tf.pow(1+tf.exp(false_logits-true_logits),-1)



def lvq2(logits,labels, min_cost):
    cost = multiclass_hinge_loss(logits, labels)
    return tf.minimum(tf.constant(min_cost), cost)


def square_square(logits,labels):
	batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_true_logits = tf.SparseTensor(indices=indices, values=logits,shape=[batch_size, num_classes])
    true_logits = tf.sparse_tensor_to_dense(sparse_true_logits)    
    opp_indices = tf.cast(tf.pack((tf.range(0, batch_size), -labels+1), axis=1), dtype=tf.int64)
    sparse_false_logits = tf.SparseTensor(indices=opp_indices, values=logits,shape=[batch_size, num_classes])
    false_logits = tf.sparse_tensor_to_dense(sparse_false_logits)    
    return tf.pow(true_logits,2)-tf.pow(tf.maximum(tf.zeros(batch_num), 1-false_logits),2)


def square_exp(logits,labels,alpha=0.5):
	batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_true_logits = tf.SparseTensor(indices=indices, values=logits,shape=[batch_size, num_classes])
    true_logits = tf.sparse_tensor_to_dense(sparse_true_logits)    
    opp_indices = tf.cast(tf.pack((tf.range(0, batch_size), -labels+1), axis=1), dtype=tf.int64)
    sparse_false_logits = tf.SparseTensor(indices=opp_indices, values=logits,shape=[batch_size, num_classes])
    false_logits = tf.sparse_tensor_to_dense(sparse_false_logits)    
    return tf.pow(true_logits,2)+alpha*(tf.exp(-false_logits))


def nll(logits, labels):
    return 0    


def mee(logits,labels):
    return 0


def modified_cosine_difference(logits, labels, alpha=0.1):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)

    '''turning targets from 0,1 to alpha,1'''
    targets = ((1-alpha)*targets) + alpha*tf.ones(num_classes)

    '''splitting 0 and 1 indexes into separate arrays'''
    a_logits, b_logits = tf.unpack(logits, axis=1)
    a_targets, b_targets = tf.unpack(targets,axis=1)

    targets_magnitude = tf.sqrt(tf.reduce_sum(tf.pow(targets,2),1))
    logits_magnitude = tf.sqrt(tf.reduce_sum(tf.pow(logits,2),1))
    total_magnitude = targets_magnitude*logits_magnitude
    return tf.pow(((b_targets*b_logits)+(a_targets * a_logits))/total_magnitude, -1)

#same as cross entropy for us
'''def log_loss(logits,labels):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_logits_correct_class = tf.SparseTensor(indices=indices, values=logits,shape=[batch_size, num_classes])
    logits_correct_class = tf.reduce_sum(tf.sparse_tensor_to_dense(sparse_logits_correct_class),1)
    opp_indices = - indices + [1,1]
    sparse_logits_incorrect_class = tf.SparseTensor(indices=opp_indices, values=logits, shape=[batch_size, num_classes])
    logits_incorrect_class = tf.reduce_sum(tf.sparse_tensor_to_dense(sparse_logits_incorrect_class),1)
    return tf.log(1+tf.reduce_sum(tf.exp(logits_correct_class - logits_incorrect_class),axis=1))
'''

def with_memory(logits, labels, func, prev_cost,alpha=0.125):
	#*args
	'''prev_cost should be initialized to tf.Variable(0)'''
    unprocessed_cost = func(logits, labels)
    return (alpha*unprocessed_cost)+((1-alpha)*prev_cost)

#not smooth, loss diverged with NaN
#used for regression?
def multiclass_rmse(logits, labels):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)
    return tf.sqrt(tf.reduce_mean(tf.square(tf.reduce_sum(logits - targets, 1))))
    #tf.contrib.losses.mean_squared_error(y_conv, y_)

def minimum_squared_error(logits, labels):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)
    return 0.5*tf.pow(tf.reduce_sum(logits - targets,1),2)

def perceptron_loss(logits, labels):	
	'''
	perceptron loss: old learning optimizer that uses logits - targets as learning rate 

	perceptron_loss = E(correct answer) - minimum E over all classifications
	where E(correct answer) = targets
	'''
	batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)

    '''turning targets from 0,1 to -1,1'''
    targets = (2*targets) - tf.ones(num_classes)

    '''splitting 0 and 1 indexes into separate arrays'''
    a_logits, b_logits = tf.unpack(logits, axis=1)
    a_targets, b_targets = tf.unpack(targets,axis=1)
    
    return 0.5*tf.maximum(tf.zeros(batch_size), (b_targets*b_logits)-(a_targets * a_logits))


#unfinished
def difference_cost(parameters_array, parameters_array2, func1, func2):
	return 0

def multiclass_hinge_loss(logits, labels):
	'''
	hinge loss: classifies bond as positive or negative with a difference of at least 1 between the most incorrect answer
	(the incorrect answer with the lowest value) and the correct answer 
	multiclass_hinge_loss = max(0, m+E(correct answer)-E(incorrect answer))
		where m is the margin, E(correct answer) = 1*logit of true classification, 
		and E(incorrect answer) = -1 * logit of false classification
	'''
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)

    '''turning targets from 0,1 to -1,1'''
    targets = (2*targets) - tf.ones(num_classes)

    '''splitting 0 and 1 indexes into separate arrays'''
    a_logits, b_logits = tf.unpack(logits, axis=1)
    a_targets, b_targets = tf.unpack(targets,axis=1)
    
    return 0.5*tf.maximum(tf.zeros(batch_size), tf.ones(batch_size) + (b_targets*b_logits)-(a_targets * a_logits))
    #tf.contrib.losses.smoothed_hinge_loss(y_conv, y_)

def sigmoid_cross_entropy(logits, labels):
	'''
	sigmoid cross entropy: similar to softmax cross entropy but allows for an input to be
	of more than one class 

	sigmoid_cross_entropy = y_i*log(sigmoid(x_i)) + (1-y_i)*log(1-sigmoid(x_i)) for i = 0,1
	'''
    labels = tf.cast(labels, dtype=tf.int32)
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)
    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, targets))


    #max_logits_index = tf.cast(tf.pack((tf.range(0, batch_size), tf.argmax(logits,1)), axis=1), dtype=tf.int64)
    #sparse_max_logits = tf.SparseTensor(indices=max_logits_index, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    #max_logits = tf.reduce_sum(tf.sparse_tensor_to_dense(sparse_max_logits), 1)
    #return 0.5*tf.reduce_mean(tf.cast(tf.maxium(tf.zeros(batch_size), labels),tf.float32))
