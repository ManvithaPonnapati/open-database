#!/usr/bin/env python123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# -*- coding: utf-8 -*-123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom __future__ import absolute_import123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom __future__ import division123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom __future__ import print_function123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport os123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport sys123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom random import randint123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom six.moves import xrange  # pylint: disable=redefined-builtin123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom utils import print_progress_bar123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# The image size to crop the image to.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# Keep at 100 to avoid any cropping;123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# do not set this value below 80123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFIMAGE_SIZE = 100123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# Global constants describing the MSHAPES data set.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFNUM_CLASSES = 2  # Binary classification123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef read_mshapes(filename_queue):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Reads a pair of MSHAPE records from the filename queue.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param filename_queue: the filename queue of lock/key files.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: a duple containing a correct example and an incorrect example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    file_pair1 = filename_queue.dequeue()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    file_pair2 = filename_queue.dequeue()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # TODO: Make sure the filenames get enqueued back and not just taken out123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    _, key_image = decode_mshapes(file_pair1[1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    _, lock_image = decode_mshapes(file_pair1[0])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    _, wrong_key_image = decode_mshapes(file_pair2[1])  # The key from the next pair in the queue123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # will not match the current lock123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Combine images to make a correct example and an incorrect example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    correct_example = tf.concat([lock_image, key_image], axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    wrong_example = tf.concat([lock_image, wrong_key_image], axis=0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Return the examples123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return correct_example, wrong_example123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef decode_mshapes(file_path):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Decodes an MSHAPE record.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param file_path: The filepath of the png123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return: A duple containing 0 and the decoded image tensor123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # read the whole file123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    serialized_record = tf.read_file(file_path)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # decode everything into int32123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    image = tf.image.decode_png(serialized_record)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Cast to float32123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    image = tf.cast(image, tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # "Crop" the image.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # This does not actually do anything, since123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # the image remains the same size; however,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # it has the effect of setting the tensor123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # shape so that it is inferred correctly123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # in later steps. For details, please123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # see https://stackoverflow.com/a/35692452123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    image = tf.random_crop(image, [100, 100, 3])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return 0, image123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef inputs(eval_data, data_dir, batch_size):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Constructs the input for MSHAPES.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param eval_data: boolean, indicating if we should use the training or the evaluation data set123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param data_dir: Path to the MSHAPES data directory123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :param batch_size: Number of images per batch123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    :return:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 6] size123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        labels: Labels. 1D tensor of [batch_size] size.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    filequeue = tf.FIFOQueue(capacity=100000, dtypes=[tf.string, tf.string])  # FIXME: Use RandomShuffleQueue instead!!123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    enqueues = []123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    if not eval_data:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Not eval data")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print_progress_bar(0, 3000, prefix='Progress:', suffix='Complete', length=50, fill='█')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for i in xrange(1, 3000):  # TODO: First of all, this should go to (at least) 30000.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                # The reason it's at 5000 is that currently, we're123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                # individually enqueueing images. Instead, we should123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                # use enqueue_many with an inline for loop, which123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                # should building up the queue much faster.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            print_progress_bar(i + 1, 3000, prefix='Progress:', suffix='Complete', length=50, fill='█')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            lock = os.path.join(data_dir, 'images/%d_L.png' % i)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            key = os.path.join(data_dir, 'images/%d_K.png' % i)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            a = filequeue.enqueue([lock, key])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            enqueues.append(a)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("Ok")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        for i in xrange(30001, 49500):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            lock = os.path.join(data_dir, 'images/%d_L.png' % i)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            key = os.path.join(data_dir, 'images/%d_K.png' % i)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            a = filequeue.enqueue([lock, key])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            enqueues.append(a)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Finished enqueueing")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Get the correct and incorrect examples from files in the filename queue.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    c, w = read_mshapes(filequeue)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Got examples")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Ensure that the random shuffling has good mixing properties.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Mixing properties stuff")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    min_fraction_of_examples_in_queue = 0.4123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    min_queue_examples = int(num_examples_per_epoch *123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                             min_fraction_of_examples_in_queue)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Generating and returning batch")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Generate a batch of images and labels by building up a queue of examples.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return (enqueues, _generate_image_and_label_batch(c, [1],  # FIXME: Also add incorrect examples to the batch123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                     min_queue_examples, batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                     shuffle=False))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef _generate_image_and_label_batch(image, label, min_queue_examples,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                    batch_size, shuffle):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """Construct a queued batch of images and labels.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Args:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      image: 3-D Tensor of [height, width, 3] of type.float32.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      label: 1-D Tensor of type.int32123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      min_queue_examples: int32, minimum number of samples to retain123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        in the queue that provides of batches of examples.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      batch_size: Number of images per batch.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      shuffle: boolean indicating whether to use a shuffling queue.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Returns:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      images: Images. 4D tensor of [batch_size, height, width, 3] size.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF      labels: Labels. 1D tensor of [batch_size] size.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Create a queue that shuffles the examples, and then123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # read 'batch_size' images + labels from the example queue.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    num_preprocess_threads = 16123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    if shuffle:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images, label_batch = tf.train.shuffle_batch(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            [image, label],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            batch_size=batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            num_threads=num_preprocess_threads,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            capacity=min_queue_examples + 3 * batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            min_after_dequeue=min_queue_examples)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        images, label_batch = tf.train.batch(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            [image, label],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            batch_size=batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            num_threads=num_preprocess_threads,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            capacity=min_queue_examples + 3 * batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Display the training images in the visualizer.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.summary.image('images', images)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    print("Images dimensions: ", images.get_shape())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    return images, tf.reshape(label_batch, [batch_size])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF