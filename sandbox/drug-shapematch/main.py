#!/usr/bin/env python123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF# -*- coding: utf-8 -*-123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom __future__ import print_function123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom __future__ import print_function123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom __future__ import print_function123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom __future__ import print_function123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport argparse123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport sys123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport time123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport numpy as np123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport tensorflow as tf123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport dsminput123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFfrom utils import *123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFsys.path.append('../../')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFimport affinity as af123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef train():123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    This function trains the network on actual drugs.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    header("Running train() function.")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Make logging very verbose123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Setting verbosity to maximum value")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.logging.set_verbosity(tf.logging.DEBUG)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Launch a tensorflow session123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Launching tensorflow session")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    sess = tf.Session()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create a filename queue first123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # filename_queue, examples_in_database = dsminput.index_database_into_q(CONSTANTS.database_path, shuffle=True)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # create an epoch counter123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # with tf.variable_scope("epoch_counter"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #     batch_counter = tf.Variable(0)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #     batch_counter_increment = tf.assign(batch_counter, tf.Variable(0).count_up_to(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #         np.round((examples_in_database * CONSTANTS.num_training_epochs) / CONSTANTS.batch_size)))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #     epoch_counter = tf.div(batch_counter * CONSTANTS.batch_size, examples_in_database)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Create the image and label batches for the data123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Creating image and label batches")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    (filequeue, (image_batch, label_batch)) = dsminput.inputs(data_dir=CONSTANTS.database_path,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                              batch_size=CONSTANTS.batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                              side_pixels=CONSTANTS.side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                              num_threads=CONSTANTS.num_threads)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # _, current_epoch, label_batch, image_batch = dsminput.image_and_label_q(batch_size=CONSTANTS.batch_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #                                                                         pixel_size=CONSTANTS.pixel_size,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #                                                                         side_pixels=CONSTANTS.side_pixels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #                                                                         num_threads=CONSTANTS.num_threads,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #                                                                         filename_q=filename_queue,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #                                                                         epoch_counter=epoch_counter,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #                                                                         lig_frame='OVERSAMPLING')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Image batch shape")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info(str(image_batch.get_shape()))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Making keep probability placeholder")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    keep_prob = tf.placeholder(tf.float32)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.variable_scope("network"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Predict the labels using a MaxNet123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        info("Setting up MaxNet")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        predicted_labels = af.networks.MaxNet().compute_output(image_batch, keep_prob, CONSTANTS.batch_size)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Calculate cross-entropy loss123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Setting up losses")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label_batch, logits=predicted_labels)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    cross_entropy_mean = tf.reduce_mean(cross_entropy)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.summary.scalar('cross entropy mean', cross_entropy_mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Randomly shuffle along the batch dimension and calculate the shuffled error123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Setting up shuffled losses")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    shuffled_labels = tf.random_shuffle(label_batch)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    shuffled_cross_entropy_mean = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=predicted_labels,123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                                                                labels=shuffled_labels))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.summary.scalar('shuffled cross entropy mean', shuffled_cross_entropy_mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Minimize cross entropy using an AdamOptimizer123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Setting up AdamOptimizer")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    with tf.variable_scope("Adam_optimizer"):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        train_step_run = tf.train.AdamOptimizer(CONSTANTS.learning_rate).minimize(cross_entropy)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Merge all summaries, create a file writer object123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Merging summaries, creating file writer")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    merged_summaries = tf.summary.merge_all()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    train_writer = tf.summary.FileWriter((CONSTANTS.summaries_dir + '/' + CONSTANTS.run_name + "_logs"), sess.graph)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Create saver to save and load the network state123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Creating saver")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    saver = tf.train.Saver(var_list=(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="Adam_optimizer") +123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                     tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="network") +123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                     tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="epoch_counter")))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Either load the saved session or initialize global variables123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Initializing/loading variables")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    if CONSTANTS.saved_session is None:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        info("Initializing variables (not loading from previous netstate)")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        sess.run(tf.global_variables_initializer())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    else:123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        info("Restoring variables from saved session")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # sess.run(tf.global_variables_initializer())123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        saver.restore(sess, CONSTANTS.saved_session)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        info("Restored variables. Beginning training")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Launch all threads123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # (Note: only do this after the graph is complete and all the variables initialized!)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Launching threads and queue runners")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    coord = tf.train.Coordinator()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    threads = tf.train.start_queue_runners(sess=sess, coord=coord)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Training loop123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Entering training loop")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    for i in xrange(1, 1000000):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        start = time.time()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        c_entropy_mean, _ = sess.run([cross_entropy_mean, train_step_run],123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                     feed_dict={keep_prob: CONSTANTS.keep_probability})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        send_cross_entropy_to_poor_mans_tensorboard(c_entropy_mean)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        sys.stdout.write("\033[93mlocal step: " + str(i)),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        sys.stdout.write("      cross entropy mean: " + rightzpad(c_entropy_mean, 8)),123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        sys.stdout.write(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            "      examples per second: " + (str(round((CONSTANTS.batch_size / (time.time() - start)), 2)) + "")[123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                            ::-1].zfill(6)[::-1])123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        print("")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        # Once in a while, save the network state123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        if (i % CONSTANTS.email_and_save_every_n_steps == (CONSTANTS.email_and_save_every_n_steps - 1)):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            c_entropy_mean, sc_entropy_mean, summaries = sess.run(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                [cross_entropy_mean, shuffled_cross_entropy_mean, merged_summaries], feed_dict={keep_prob: 1})123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            notify("Hi, " + CONSTANTS.nickname + "!\n\n"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                                                 "While you were away, I have gone through "123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   + str(i + 1) + " training steps.\nThe current cross-entropy mean is " + str(c_entropy_mean) + ", and"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   + " the shuffled cross-entropy mean is " + str(sc_entropy_mean) + ".\n\n\nThanks! ❤",123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                   subject="Training step " + str(i + 1))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            printi(123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                "cross entropy mean: " + str(c_entropy_mean) + "; shuffled cross entropy mean: " + str(sc_entropy_mean))123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            train_writer.add_summary(summaries)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            saver.save(sess, CONSTANTS.summaries_dir + '/' + CONSTANTS.run_name + "_netstate/saved_state",123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF                       global_step=i)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            info("Emailed information and saved net state.")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    assert not np.isnan(cross_entropy_mean), 'Model diverged with loss = NaN'123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFdef main(_):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    Creates the directories and begins training.123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    """123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    header("Main function")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Running main function.")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    notify_constants()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Checking dataset...")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    check_dataset(data_dir=CONSTANTS.database_path)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Dataset ok.")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Directories stuff123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    #123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Setting up directories...")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    summaries_dir = os.path.join(CONSTANTS.summaries_dir)123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # CONSTANTS.run_index defines when123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    CONSTANTS.run_index = 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    CONSTANTS.run_name = CONSTANTS.run_name + "_main"123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    while tf.gfile.Exists(summaries_dir + "/" + str(CONSTANTS.run_index) + '_netstate') \123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF            or tf.gfile.Exists(summaries_dir + "/" + str(CONSTANTS.run_index) + '_logs'):123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF        CONSTANTS.run_index += 1123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    CONSTANTS.run_name = str(CONSTANTS.run_index) + "_" + CONSTANTS.run_name123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.gfile.MakeDirs(summaries_dir + "/" + CONSTANTS.run_name + '_netstate')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.gfile.MakeDirs(summaries_dir + "/" + CONSTANTS.run_name + '_logs')123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Done")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    # Start training the network!123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    info("Starting training")123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    train()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF123343DJNBFHJBJNKFJNBHDRFBNJKDJUNFif __name__ == '__main__':123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF    tf.app.run()123343DJNBFHJBJNKFJNBHDRFBNJKDJUNF